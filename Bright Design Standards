Getting Started

Checking Chrome + Computer Mic + Video Settings
When new users - particularly users who may be behind a VPN - first access Bright, they sometimes discover that audio and video recording functions are not working. Read on to learn how to adequately tech test your system for a seamless experience.
Chrome Settings
The best place to start is the Chrome settings associated with the Bright platform address. To check these, first have the user click the small lock icon next to the html link and select 'Site settings.'

9f4e159b-0106-4075-a156-a3085e3acd17 Next confirm that the 'Camera,' 'Microphone,' and 'Sound' settings are all set to 'Allow' as noted below. 

db0a5f1b-7af5-46ac-97ae-7200f85d60ed

Microphone Settings
If recording functionality still is not working, you can check which microphone your browser is using for Bright. To do this open your Chrome microphone settings - chrome://settings/content/microphone.
First, confirm that the Bright app is listed under 'Allowed to use your microphone.'
943871ab-a1dc-4661-9e14-08b9762dbc98
Second, check which microphone is selected for use in the browser. As noted in the sample image below, sometimes computers have multiple microphone inputs. This is especially common in customer service settings where agents use a headset microphone separately from their built-in computer microphone. We have seen a couple scenarios where agents could not record, and then when they plugged in their headsets were able to proceed.

Computer Permissions
The final (though less common) issue to check is the individual user's computer settings. In some cases companies or VPN networks disable access to certain apps by default. Follow the steps in the images below to confirm Chrome mic access permissions for both PC and Mac computers. 

PC
Step 1: Visit Settings and select Privacy

Step 2: Choose Microphone

Step 3: Scroll Down and confirm that the toggle is switched to the 'On' position and that Chrome is included under 'Allow desktop apps to access your mic.'

MAC
Step 1: Visit Settings and select 'Privacy & Security'
Screenshot 2023-02-23 at 10.23.33 AM
Step 2: Choose 'Camera' and Allow Google Chrome
Screenshot 2023-02-23 at 10.22.45 AM
Step 3: Repeat steps 1 and 2 for your microphone as shown below.
Screenshot 2023-02-23 at 10.22.58 AM
If these steps don't work, we recommend you work with your Bright account manager to submit a formal ticket to your company's IT support team.



Understanding Moment Types
Below is a summary of each Moment Type that you will find in Bright.
Each individual activity for learners is what we call a 'Moment.' Moments are bite-sized learning experiences that provide learners with a chance to practice key skills. Depending on the nature of the learning objectives for your lesson, your team may choose to use different types of moments to include in your learning journey.

Remember that a Moment Type indicates what you want the learner to do. So when choosing or deciding which Moment Type you want to use, ask yourself: What do I want the learner to do in this Moment? Do I want them to record an audio or written response, answer questions, or navigate a system?

Below is a quick summary of all the Moment Types, as well as the learning objective and metrics that collected in each. You can click the moment type heading to open the article explaining how to build that moment type.

Audio Recording
Possible Use Cases

Learner watches video explaining consistency in our opening and practices our standard greeting
Learner listens to a conversation clip and practices empathizing
Learner reviews screenshot of our CRM tool and explains options to client
Learner reviews PDF file and explains the billing options to a customer
Can be AI coached
Learner must have access to a microphone to submit

 
Writing
Possible Use Cases

Learner watches a video and practices writing notes for a given interaction
Learner listens to call clip then practices writing an email follow-up to a client 
Learner reviews screenshot of client order form then practices writing making a sales request
Learner reviews PDF of current insurance policy and writes whether this meets our minimum criteria and why
Can be AI coached

 
System
Possible Use Cases

Learner practices navigating the primary software from their day to day
Learner listens to an interaction while being the one to navigate the screens and fill out the form
Learner practices creating/escalating tickets
Learner practices creating a new patient record
Can set thresholds for duration (AHT), number of errors, and number of lifelines

Requires a canvas to be built- read up on how to build a canvas here
 
Video
Possible Use Cases

Learner watches a video from a patient then records a tele-health response
Learner listens to a spoken sentence and records themselves translating to ASL
Learner reviews image of registration card and records their interaction for front desk role
Learner reviews PDF and records video mock interview answers for leadership role
Requires Human Coach- so consider bandwidth within your organization

Learner must have access to a microphone AND video camera to submit
 

Document Upload
Possible Use Cases

Learner watches video lesson on navigating the Knowledge Base then navigates + uploads screenshot of proper article
Learner listens to audio file and fills out and uploads PDF order form
Learner reviews image of current metrics and uploads excel file with sample data and formulas
Learner reviews PDF of duty roster and uploads word doc with consolidated weekly duty report
Requires Human Coach- so consider bandwidth within your organization

 

SCORM 
Possible Use Cases

Learner completes SCORM file to be introduced to concept (Learn part of Learn, See, Do)
Learner completes interactive SCORM with vocabulary flashcards
Learner completes SCORM lesson with assessment elements
Learner completes SCORM lesson for annual compliance training
Bright currently hosts SCORM, but is not an authoring tool for SCORM

Multiple Choice
 Possible Use Cases

Learner watches video explaining core concept and completes knowledge check
Learner listens to a conversation and answers multi-choice questions about the interaction
Learner reviews screenshot a new service and answers potential questions from customers
Learner reviews PDF file and answers questions about the information 
Typically not our first choice for a moment type because it usually provides the least, hands-on experience

Unable to tag this moment type with a skill 

 
Wrap-Up on Moment Types
The takeaway here is to reinforce the moment type is what the learner will be doing. Barring System and SCORM moments- any moment type can use any of our media options before responding. Whether watching a video, listening to audio clips, viewing images, or reading a PDF, the learner can practice through Audio, Writing, Video Recording, or Document Upload. This variety of moment types is meant to help you recreate the real situations your learners will experience in their daily roles.
 
Remember to choose your moment type based on the learner's role. If they are a client-facing role, you should have more audio moments than a learner who works in the back-office.



Screen Capture + Recording Tips
Below are a few recommendations on tools that you can use to elevate your skills and that will help you build high-resolution System Simulations
In order to create a quality, high-resolution System Moment, your team will need to select a screen capture and recording tool, as well as a video editing tool. Since these tools operate  outside the Bright Platform, you can pick whatever tool you prefer. However, we do have a few recommendations on features to prioritize. When Bright creates Sims and Moments, we use a mix of tools depending on the situation, including: 

Snipping Tool: This tool comes pre-loaded on most PCs, is free, and is very easy to use. 
PicPick: Also free, this tool can do a mix of screen captures, including the Window Scroll Capture. 
Chrome Full Page Screen Capture: There is a great, free Chrome Plugin that captures full window screen captures, including Window Scroll Capture. A great option if your CRM is accessed via browser
Camtasia: This is a video and screen recording software that can be used to support both System Canvas creation, as well as Simulation/Moment file creation. Camtasia also has many features that you can use to create non-simulation content, such as video walkthroughs with knowledge checks. 
Filmora: This is a video and screen recording alternative to Camtasia (though we like Camtasia more!)
Photoshop: This is not essential, but those who are interested may wish to use Photoshop or a similar image editing tool in some cases. Most image editing needs can be met in Bright's System Canvas builder tool. However, complicated configuration settings for some centers may require certain edits to be made before uploading to Bright. Photoshop licenses can be purchased from Adobe for $9.99/month per license.
Production + Consistency Tips
Key factors to avoid re-work and annoying inconsistencies in your System Canvas include: 

Computer Settings: It's not always visible to the naked eye, but most computers use different screen resolution settings. This is why it is VERY important that you capture your screenshots on either a very good and big computer screen, OR on exactly the same computer type/settings that your trainees will use. The key phrase here is you can always reduce screenshot resolution and quality, but you can not increase it. This can become noticeable if you take screenshots on multiple computers and then try to blend them together in a single System Moment. Screens can become pixelated in the process, which is usually distracting to the learner. 
Consistency: Related to resolution is consistency in screenshot capture location. The upper left corner should be EXACTLY the same in every screenshot. Being off by even a couple pixels can cause the screen to "move around" as the learner clicks through. Avoid using the free hand screen shot option in favor of the Window or Scrolling Window options, which automatically address this by capturing the exact same window area every time. 
Width: Finally, be sure to leave your system screen open to the same width throughout all your capture sessions - ideally full screen on a screen the same size as your future trainees. Bright's Builder tool will automatically re-size screens down, but it can still feel "glitchy" when screenshots start at different widths and sizes and are then normalized. 
Advanced Tips!
One last screen capture tip for Advanced Users is directly editing Chrome or IE UX code in your browser before capturing a screen shot.
The System Moment Builder tool allows you to customize any type of system data in the Bright Platform, creating a dynamic system experience. For legal reasons, you'll need to anonymize data before creating a sim either way; however, there may be times when you wish to change data for the baseline screen BEFORE you take the screen shot. This may look advanced, but really it's just a classic Ctrl+F inside some HMTL code. But the outcomes are really cool, fun and helpful. 

Step 1: Go to screen you want to edit:
Start by going to the system screen you want to edit. This must be in a modern browser like Chrome to work. In this case we'll go to The Washington Post newspaper site.

 
Step 2: Right Click near the item you want to change:
To make the text/site change, right click near the area you want to change. In this case, I don't like the Coronavirus story - it makes me too sad. So I want to change it to something more positive. So I right click near that story as shown below.

 
Step 3: Edit the Text in the HTML Bar:
Next, you'll see a very fancy HMTL Inspector frame come up on the right side of the screen. DON'T PANIC! You don't really need to know how to code as long as you can 1) visually find the text phrase you want to change or 2) use Ctrl+F. In this case, I can see the Coronavirus headline in the text already selected; that's why we told you to right-click near the thing you want to edit. You can now click on the text and change it to whatever you want. I'll type "Bright is the Best Learning Company in the World."

 
Step 4: Click anywhere outside the Inspector: 
And there you have it. It's official on the Washington Post home page: Bright is the Best Learning Company in the World. What just happened? Your browser referenced the original code when you first visited, but then essentially downloaded a temporary copy. When you changed the text, you changed the temporary copy, not the original. But your screenshot - and future learner - won't know that. By editing within the HTML you've kept all the placement and formatting, with very little work. You can go practice on basically any website you want. Have fun!



Audio Recording Moment Creation
Step-by-step instructions on how to create a moment where the learner has to submit an audio recording.
First, name your Moment.

From the Moments menu in the Admin console, select the 'Audio Recording' box as the Moment Type. 

Select the 'Media Type'. This is the piece of content that the learner will view and/or analyze before answering the learner prompt or question. This can either be a Video file (.mp4, .avi, .mwv), Audio file (.mp3, .wav, .m4a) or Image (.jpeg or .png files). 

Once you select the media type, drag the file into the media section or browse your computer to add the file.
Next, enter the Duration. This is the time it will take the average learner to complete the exercises. It's important to leave extra time for them to the answer questions at the end after they've viewed or analyzed the media. Keep in mind learners will likely record their response, review the response, and may want to re-do the recording. For example, if they are expected to watch a 5 minute video and then record a 1-2 minute response, you would like set the Duration to 6-7 Minutes. Lastly, the Duration does not have to be listed in a particular format (ie. 00:00 or 5 Minutes). This is a text field so you can type whatever format makes sense. Just be sure to be consistent throughout the building process. 

Now enter the Short and Long description. The short description is mostly for the Admins and Coaches who will use this summary as a way to quickly see what the moment entails while building simulations or perusing the reports. The long description will be learner-facing prompt and should include the instructions about what you want the learner to do in this moment. It's important to be descriptive in the Long description to give as much detail and direction to the learner.

Enter the Coach Guidance. This is an important step to ensure rating consistency across the team of coaches who will be giving your learners feedback on this moment. The coaching guidance should align to your quality standards and expectations for this exercise. They will be expected to give a 4-star rating so you should also include detail about what each star means in this moment. Remember that 3 or 4 stars is considered passing and a 1 or 2 star rating will require the learner to resubmit a response. Lastly, we recommend including the learner prompt in your coach guidance so they can easily reference the details of prompt and what you've asked the learner to do. If you need more information or best practices, please see our article on Coaching Basics.

Assign a Team Tag. This is not required however we do recommend tagging your content to the specific Admin team that this content is intended for. As you and your organization grow with the platform, this is the easiest way to organize content for your division, region, etc. To learn more about content tagging, see this related article.

Next, add a Hero Image. The hero image will be the backsplash image that is shown behind the piece of media. If you do not add a Hero image, this will just be a blank, grey screen so we recommend always adding this to every moment type. The preferred dimensions for Hero images is 1920x1080px and we support .jpeg, .jpg, .png files. If you are struggling to find images, please contact your account manager who can guide you to the Bright Image library where you will find a large collection of perfectly sized images for every place in the platform. 

Only add a Thumb image if your Moment will be pinned to the Home Page. You have ability to highlight Moments that you want to live on the Home Page. These moments might be exercises that you want learners to do over and over again. If you'd like your moment to live on the Home Page, be sure to toggle this option in the top right corner when creating the moment. 
Homepage Moment Toggle
Thumb images should be 800x1200 and we support the same file types as the Hero Image. The Bright team recommends leaving a 800x200 white space at the top of the image so that it fits nicely into the vignette that learners will see on the Home Page. Again, ask your account manager for assistance or direction to the Bright Image library if you'd like additional help here.
Next, click 'Save' to create the moment. A green confirmation box will appear in the lower left corner of the page to let you know that the moment was created successfully.
Once the moment has been created successfully, you will need to tag the moment with Skills. Click the 'Skills + AI' tab at the top.

Click Add a Skill / Rating.

Select from the Skills in your library. You can select multiple skills for an individual moment and the coaches will be expected to give a 4-star rating against each skill. For example, you might select a skill related to their 'Grammar' and also select a skill related to their 'Product Knowledge'. Someone might have the product knowledge but the way they delivered the information could be clearer or more concise. You can add multiple skills by repeating steps 13-14. If you do not see any skills, this means that you have not added any to your library. You will have to navigate to the Skills menu and add skills before being able to assign skills to this moment. See more information about this in our Skills Library article.

Lastly, Writing and Audio Recording moments have the option to be rated by the Bright AI-Coach. If there is a somewhat standard answer for this moment or certain terms that you would expect every learner to include in their submission, we would recommend turning on the AI-Coach. If each answer for this submission will be nuanced and require a human coach, you can select Save + Close to return to the Moments menu. Human coaching will lean more heavily on the information in the coach guidance.

To turn the AI-Coach on select, 'Add a rating rule' under the skill that you have selected. This will open the AI building tool. For more details on how to craft AI rules, please see our article on AI Rule Creation and Best Practices.



Writing Moment Creation
Step-by-step instructions on how to create a moment with a writing exercise at the end.
First, name your Moment.

From the Moments menu in the Admin console, select the 'Multi Choice / Writing' box as the Moment Type. 

Select the 'Media Type'. This is the piece of content that the learner will view and/or analyze before answering the prompt/questions. This can either be a Video file (.mp4, .avi, .mwv), Audio file (.mp3, .wav, .m4a) or Image (.jpeg or .png files). 

Once you select the media type, drag the file into the media section or browse your computer to add the file.
Next, enter the Duration. This is the time it will take the average learner to complete the exercises. It's important to leave extra time for them to the answer questions at the end after they've viewed or analyzed the media. For example, if they are expected to watch a 5 minute video, do some research and then come back and give a 2-3 sentence response, you would like set the Duration to 10-11 Minutes in order to account for the extra time to finish the exercise. Lastly, the Duration does not have to be listed in a particular format (ie. 00:00 or 5 Minutes). This is a text field so you can type whatever format makes sense. Just be sure to be consistent throughout the building process. 

Now enter the Short and Long description. The short description is mostly for the Admins and Coaches who will use this summary as a way to quickly see what the moment entails while building simulations or perusing the reports. The long description will be learner-facing and show up on the Simulation page and the Moment landing page prior to them starting the exercise. It's important to be descriptive in the Long description to give as much detail and direction to the learner.

Enter the Coach Guidance. This is an important step to ensure rating consistency across the team of coaches who will be giving your learners feedback on this moment. The coaching guidance should align to your quality standards and expectations for this exercise. They will be expected to give a 4-star rating so you should also include detail about what each star means in this moment. Remember that 3 or 4 stars is considered passing and a 1 or 2 star rating will require the learner to resubmit a response. Lastly, we recommend including the learner prompt in your coach guidance so they can easily reference the details of prompt and what you've asked the learner to do. If you need more information or best practices, please see our article on Coaching Basics.

Assign a Team Tag. This is not required however we do recommend tagging your content to the specific Admin team that this content is intended for. As you and your organization grow with the platform, this is the easiest way to organize content for your division, region, etc. To learn more about content tagging, see this related article.

Next, add a Hero Image. The hero image will be the backsplash image that is shown behind the piece of media. If you do not add a Hero image, this will just be a blank, grey screen so we recommend always adding this to every moment type. The preferred dimensions for Hero images is 1920x1080px and we support .jpeg, .jpg, .png files. If you are struggling to find images, please contact your account manager who can guide you to the Bright Image library where you will find a large collection of perfectly sized images for every place in the platform. 

Only add a Thumb image if your Moment will be pinned to the Home Page. You have ability to highlight Moments that you want to live on the Home Page. These moments might be exercises that you want learners to do over and over again. If you'd like your moment to live on the Home Page, be sure to toggle this option in the top right corner when creating the moment. 
Homepage Moment Toggle
Thumb images should be 800x1200 and we support the same file types as the Hero Image. The Bright team recommends leaving a 800x200 white space at the top of the image so that it fits nicely into the vignette that learners will see on the Home Page. Again, ask your account manager for assistance or direction to the Bright Image library if you'd like additional help here. 
Next, click 'Save' to create the moment. A green confirmation box will appear in the lower left corner of the page to let you know that the moment was created successfully. Now, select the 'Prompts' tab at the top of the page.

Select the Prompt Action as 'Write'.

Next, enter the learner prompt and a detailed description of what you'd like the learner to respond to in the 'Prompt Text' field. We recommend giving as much detail as possible here. We also encourage you to include information about the steps they should take to submit their answer and the expectations for how long it will take for a coach to give them asynchronous feedback. 

For example, you might ask the learner to analyze an image of an insurance claim and respond in writing whether or not the member qualifies for a certain benefit and why. After this prompt, you might also include something like: 

"Once you have written your response and you are happy with the details, click 'Submit to Coach'. Our coaching team will provide detailed feedback within the next 24 hours. If you see your alert bell highlighted on the homepage, you can navigate to My Path to read your rating and feedback. We use a 4-star rating scale and you must receive 3 or 4 stars to pass. If you receive 1 or 2 stars, you will be expected to resubmit your response. Good luck!"

Once you've completed the prompt, you will need to tag the moment with Skills. Click the 'Skills + AI' tab at the top.

Click Add a Skill / Rating.

Select from the Skills in your library. You can select multiple skills for an individual moment and the coaches will be expected to give a 4-star rating against each skill. For example, you might select a skill related to their 'Grammar' and also select a skill related to their 'Product Knowledge'. Someone might have the product knowledge but there are mistakes in spelling or syntax. You can add multiple skills by repeating steps 15-16. If you do not see any skills, this means that you have not added any to your library. You will have to navigate to the Skills menu and add skills before being able to assign skills to this moment. See more information about this in our Skills Library article.

Lastly, Writing and Audio moments have the option to be rated by the Bright AI-Coach. If there is a somewhat standard answer for this moment or certain terms that you would expect every learner to include in their submission, we would recommend turning on the AI-Coach. If each answer for this submission will be nuanced and require a human coach, you can select Save + Close to return to the Moments menu. Human coaching will lean more heavily on the information in the coach guidance.

To turn the AI-Coach on select, 'Add a rating rule' under the skill that you have selected. This will open the AI building tool. For more details on how to craft AI rules, please see our article on AI Rule Creation and Best Practices.




Multiple Choice Moment Creation
Step-by-step instructions on how to create a moment with multiple choice questions at the end.
First, name your Moment.

From the Moments menu in the Admin console, select the 'Multi Choice / Writing' box as the Moment Type. 

Select the 'Media Type'. This is the piece of content that the learner will view and/or analyze before answering the multiple choice questions. This can either be a Video file (.mp4, .avi, .mwv), Audio file (.mp3, .wav, .m4a) or Image (.jpeg or .png files). 

Once you select the media type, drag the file into the media section or browse your computer to add the file.
Next, enter the Duration. This is the time it will take the average learner to complete the exercises. It's important to leave extra time for them to the answer questions at the end after they've viewed or analyzed the media. For example, if they are expected to watch a 5 minute video and then answer 3-4 multiple choice questions, you would like set the Duration to 7-8 Minutes. Lastly, the Duration does not have to be listed in a particular format (ie. 00:00 or 5 Minutes). This is a text field so you can type whatever format makes sense. Just be sure to be consistent throughout the building process. 

Now enter the Short and Long description. The short description is mostly for the Admins and Coaches who will use this summary as a way to quickly see what the moment entails while building simulations or perusing the reports. The long description will be learner-facing and show up on the Simulation page and the Moment landing page prior to them starting the exercise. It's important to be descriptive in the Long description to give as much detail and direction to the learner.

Coach Guidance is not necessary for Multi Choice moments since the Coaches will not be giving star ratings or coaching these moment types. This is essentially an automatic pass/fail exercise.
Assign a Team Tag. This is not required however we do recommend tagging your content to the specific Admin team that this content is intended for. As you and your organization grow with the platform, this is the easiest way to organize content for your division, region, etc. To learn more about content tagging, see this related article.

Next, add a Hero Image. The hero image will be the backsplash image that is shown behind the piece of media. If you do not add a Hero image, this will just be a blank, grey screen so we recommend always adding this to every moment type. The preferred dimensions for Hero images is 1920x1080px and we support .jpeg, .jpg, .png files. If you are struggling to find images, please contact your account manager who can guide you to the Bright Image library where you will find a large collection of perfectly sized images for every place in the platform. 

Only add a Thumb image if your Moment will be pinned to the Home Page. You have ability to highlight Moments that you want to live on the Home Page. These moments might be exercises that you want learners to do over and over again. If you'd like your moment to live on the Home Page, be sure to toggle this option in the top right corner when creating the moment. 
Homepage Moment Toggle
Thumb images should be 800x1200 and we support the same file types as the Hero Image. The Bright team recommends leaving a 800x200 white space at the top of the image so that it fits nicely into the vignette that learners will see on the Home Page. Again, ask your account manager for assistance or direction to the Bright Image library if you'd like additional help here. 
Next, click 'Save' to create the moment. A green confirmation box will appear in the lower left corner of the page to let you know that the moment was created successfully. Now, select the 'Prompts' tab at the top of the page.

Select the Prompt Action as Select a choice.

Next decide how you you want the questions to be served up. We've included this feature to give you an opportunity to randomize the questions and avoid the chance of memorization or sharing of answers among the cohorts. There are 4 options here:
All-Entered Order: this means ALL questions will be served up to the learner in the order that you enter them here.
All-Random Order: this means ALL questions will be served up to the learner but in random order every time.
80%-Random Order: this means that 80% of the questions will be served up in random order.
50%-Random Order: this means that 50% of the questions will be served up in random order.

Now select a 'Threshold to Pass'. You do not have to select a threshold but if you are entering lots of questions or using this moment type as an assessment, you likely will want set a passing score. This means that if the learner does not obtain a passing grade, we will serve the assessment up to them right away. Your options for a passing grade are: 100%, 90%, 80%, 70%, 60%, 50%.

Now you can begin adding questions! Select the plus sign next to 'Add a Question'.

Now click on the box with <Question Name> to customize it.

You can now name the question for ease of reference, but most importantly you will want to fill in the Question Prompt. This is what the learner will see above the multiple choice selections and should be the question itself. Once you've entered in the question, click the +Add Choices button.

This is where you will begin adding the different multiple choice options. Enter the first option that you want displayed to the learner in the 'Option Name' field. 

Next, enter the 'Rationale'. This is what the learner sees when they select this option. If the option is correct, you should enter WHY the answer is correct. If the option is incorrect, you should give context as to WHY the answer is incorrect. For example, if the question is 'What color is the grass?' and the option is 'Purple' and it is the Incorrect option, you might write: The color of the grass can indicate the health of the lawn, but we do not see Purple grass. Try again. 

Finally, choose the 'On select' option. Typically you will want this question to End and move on to the next question, however you do also have the option to 'Play Media' upon selection of the option. If you choose to play media, you will have the select the media type and add the file to the details. When you are finished, select Add Choice.

Repeat step 18 until you have entered all of the options for that question. Once you have finished, you must select a 'Correct Choice'. If you do not select the correct choice, there will be a warning and this moment will not allow learners to progress. Back up in the details section where you typed out the question, click on the 'Correct Choice' box and select the correct choice.

Repeat steps 15-19 for each question that you would like to add to this moment. Once you are done, click 'Save and Close' at the bottom and this will take you back to the Moment Menu in the Admin console. If you are adding an assessment, please see the steps in the Bulk Moment Import article to help you add many questions at once.

Under rare circumstances, you may want to create a "Click to Continue" type moment. Really the ONLY time this makes sense is if we want to have the learner review media (video, image, or audio) then immediately practice a system simulation. In these cases, leave the question blank and only provide one answer choice that says "Continue". This image shows the full example of the moment settings to provide the learner a simple click to continue experience. 



Video Recording Moment Creation
Step-by-step instructions on how to create a moment where the learner has to submit a video recording.
First, name your Moment.

From the Moments menu in the Admin console, select the 'Video Recording' box as the Moment Type. Be mindful this moment type requires both audio and video. If a Learner is unable to record video, they will not be able to complete this moment type.

Select the 'Media Type'. This is the piece of content that the learner will view and/or analyze before answering the multiple choice questions. This can either be a Video file (.mp4, .avi, .mwv), Audio file (.mp3, .wav, .m4a) or Image (.jpeg or .png files). 

Once you select the media type, drag the file into the media section or browse your computer to add the file.
Next, enter the Duration. This is the time it will take the average learner to complete the exercises. It's important to leave extra time for them to record a response to the prompt after they've viewed or analyzed the media. Keep in mind learners will likely record their response, review the response, and may want to re-do the recording. For example, if they are expected to watch a 5 minute video and then record a 1-2 minute response, you would like set the Duration to 6-7 Minutes. Lastly, the Duration does not have to be listed in a particular format (ie. 00:00 or 5 Minutes). This is a text field so you can type whatever format makes sense. Just be sure to be consistent throughout the building process. 

Now enter the Short and Long description. The short description is mostly for the Admins and Coaches who will use this summary as a way to quickly see what the moment entails while building simulations or perusing the reports. The long description will be learner-facing prompt and should include the instructions about what you want the learner to do in this moment. It's important to be descriptive in the Long description to give as much detail and direction to the learner.

Enter the Coach Guidance. This is an important step to ensure rating consistency across the team of coaches who will be giving your learners feedback on this moment. The coaching guidance should align to your quality standards and expectations for this exercise. They will be expected to give a 4-star rating so you should also include detail about what each star means in this moment. Remember that 3 or 4 stars is considered passing and a 1 or 2 star rating will require the learner to resubmit a response. Lastly, we recommend including the learner prompt in your coach guidance so they can easily reference the details of prompt and what you've asked the learner to do. If you need more information or best practices, please see our article on Coaching Basics.

Assign a Team Tag. This is not required however we do recommend tagging your content to the specific Admin team that this content is intended for. As you and your organization grow with the platform, this is the easiest way to organize content for your division, region, etc. To learn more about content tagging, see this related article.

Next, add a Hero Image. The hero image will be the backsplash image that is shown behind the piece of media. If you do not add a Hero image, this will just be a blank, grey screen so we recommend always adding this to every moment type. The preferred dimensions for Hero images is 1920x1080px and we support .jpeg, .jpg, .png files. If you are struggling to find images, please contact your account manager who can guide you to the Bright Image library where you will find a large collection of perfectly sized images for every place in the platform. 

Only add a Thumb image if your Moment will be pinned to the Home Page. You have ability to highlight Moments that you want to live on the Home Page. These moments might be exercises that you want learners to do over and over again. If you'd like your moment to live on the Home Page, be sure to toggle this option in the top right corner when creating the moment. 
Homepage Moment Toggle
Thumb images should be 800x1200 and we support the same file types as the Hero Image. The Bright team recommends leaving a 800x200 white space at the top of the image so that it fits nicely into the vignette that learners will see on the Home Page. Again, ask your account manager for assistance or direction to the Bright Image library if you'd like additional help here. 
Next, click 'Save' to create the moment. A green confirmation box will appear in the lower left corner of the page to let you know that the moment was created successfully.
Once the moment has been created successfully, you will need to tag the moment with Skills. Click the 'Skills' tab at the top.

Click Add a Skill / Rating.

Select from the Skills in your library. You can select multiple skills for an individual moment and the coaches will be expected to give a 4-star rating against each skill. For example, you might select a skill related to their 'Grammar' and also select a skill related to their 'Product Knowledge'. Someone might have the product knowledge but the way they delivered the information could be clearer or more concise. You can add multiple skills by repeating steps 13-14. If you do not see any skills, this means that you have not added any to your library. You will have to navigate to the Skills menu and add skills before being able to assign skills to this moment. See more information about this in our Skills Library article.

Once complete, click the Save + Close button to return to the Moments menu.




Document Upload Moment Creation
Step-by-step instructions on how to create a moment where the learner has to submit a document to a coach for review.
First, name your Moment.

From the Moments menu in the Admin console, select the 'Document Upload' box as the Moment Type. Be mindful we support all document file formats including PDF, JPEG, PNG and even mp4s and mp3s.

Select the 'Media Type'. This is the piece of content that the learner will view and/or analyze before answering the multiple choice questions. This can either be a Video file (.mp4, .avi, .mwv), Audio file (.mp3, .wav, .m4a) or Image (.jpeg or .png files). 

Once you select the media type, drag the file into the media section or browse your computer to add the file.
Next, enter the Duration. This is the time it will take the average learner to complete the exercises. It's important to leave extra time for them to find or create a document after they've viewed or analyzed the media. For example, if they are expected to watch a 5 minute video and then you ask them to go find the correct knowledge article and upload this to the moment, you'll want to account for that time and set the Duration to 10-12 Minutes. Lastly, the Duration does not have to be listed in a particular format (ie. 00:00 or 5 Minutes). This is a text field so you can type whatever format makes sense. Just be sure to be consistent throughout the building process. 

Now enter the Short and Long description. The short description is mostly for the Admins and Coaches who will use this summary as a way to quickly see what the moment entails while building simulations or perusing the reports. The long description will be learner-facing prompt and should include the instructions about what you want the learner to do in this moment. It's important to be descriptive in the Long description to give as much detail and direction to the learner. Finally, we support all document file formats including PDF, PPT, Word, Excel, JPEG, PNG and even mp4s and mp3s. 

Enter the Coach Guidance. This is an important step to ensure rating consistency across the team of coaches who will be giving your learners feedback on this moment. The coaching guidance should align to your quality standards and expectations for this exercise. They will be expected to give a 4-star rating so you should also include detail about what each star means in this moment. Remember that 3 or 4 stars is considered passing and a 1 or 2 star rating will require the learner to resubmit a response. Lastly, we recommend including the learner prompt in your coach guidance so they can easily reference the details of prompt and what you've asked the learner to do. If you need more information or best practices, please see our article on Coaching Basics.

Assign a Team Tag. This is not required however we do recommend tagging your content to the specific Admin team that this content is intended for. As you and your organization grow with the platform, this is the easiest way to organize content for your division, region, etc. To learn more about content tagging, see this related article.

Next, add a Hero Image. The hero image will be the backsplash image that is shown behind the piece of media. If you do not add a Hero image, this will just be a blank, grey screen so we recommend always adding this to every moment type. The preferred dimensions for Hero images is 1920x1080px and we support .jpeg, .jpg, .png files. If you are struggling to find images, please contact your account manager who can guide you to the Bright Image library where you will find a large collection of perfectly sized images for every place in the platform. 

Only add a Thumb image if your Moment will be pinned to the Home Page. You have ability to highlight Moments that you want to live on the Home Page. These moments might be exercises that you want learners to do over and over again. If you'd like your moment to live on the Home Page, be sure to toggle this option in the top right corner when creating the moment. 
Homepage Moment Toggle
Thumb images should be 800x1200 and we support the same file types as the Hero Image. The Bright team recommends leaving a 800x200 white space at the top of the image so that it fits nicely into the vignette that learners will see on the Home Page. Again, ask your account manager for assistance or direction to the Bright Image library if you'd like additional help here. 
Next, click 'Save' to create the moment. A green confirmation box will appear in the lower left corner of the page to let you know that the moment was created successfully.
Once the moment has been created successfully, you will need to tag the moment with Skills. Click the 'Skills' tab at the top.

Click Add a Skill / Rating.

Select from the Skills in your library. You can select multiple skills for an individual moment and the coaches will be expected to give a 4-star rating against each skill. For example, you might select a skill related to their 'Presentation' and also select a skill related to their 'Product Knowledge'. Someone might have the product knowledge but the way they presented the information in the document could be clearer or more concise. You can add multiple skills by repeating steps 13-14. If you do not see any skills, this means that you have not added any to your library. You will have to navigate to the Skills menu and add skills before being able to assign skills to this moment. See more information about this in our Skills Library article.

Once complete, click the Save + Close button to return to the Moments menu.




SCORM Moment Creation
Step-by-step instructions on how to create a moment with a SCORM file.
Building a SCORM Moment:
1. Create your eLearning lesson using an authoring tool (Storyline, Captivate, Rise, etc.)


2. Publish/export the eLearning as a SCORM file

The SCORM file must have been published in either SCORM 1.2 or SCORM 2004 2nd, 3rd, 4th Editions to be compatible.

We also recommend limiting the size of the SCORM files to be less than 50MB.

3. Open Bright, and name your Moment.



4. Select the 'SCORM' box as the Moment Type.



5. Drag and drop or browse to upload your zipped SCORM file. 

SCORM Pic 1

Once you have successfully uploaded your SCORM file, the file name will be displayed above a preview section where you can review the file directly from the Moment builder page to ensure it is the correct file intended for the upload. If this file was uploaded by mistake you can select the delete icon just above the top right of the preview and start the upload process again with the correct file.

SCORM Pic 2

6. Configure the Threshold to Pass. This should match your SCORM file settings. Note - this dropdown does not override the SCORM settings required to pass. It simply makes it visible in the reporting suite.



7. Next, enter the Duration. This is the time it will take the average learner to complete the exercises. It's important to consider all of the videos and activities within your SCORM file. The Duration does not have to be listed in a particular format (ie. 00:00 or 5 Minutes). This is a text field so you can type whatever format makes sense. Just be sure to be consistent throughout the building process. 


8. Now enter the Short and Long description. The short description is mostly for the Admins and Coaches who will use this summary as a way to quickly see what the moment entails while building simulations or perusing the reports. The long description will be learner-facing prompt and should include the instructions about what you want the learner to do in this moment. It's important to be descriptive in the Long description to give as much detail and direction to the learner.

9. Assign a Team Tag. This is not required however we do recommend tagging your content to the specific Admin team that this content is intended for. As you and your organization grow with the platform, this is the easiest way to organize content for your division, region, etc. To learn more about content tagging, see this related article.

10. Next, add a Hero Image. The hero image will be the backsplash image that is shown behind the SCORM media. If you do not add a Hero image, this will just be a blank, grey screen so we recommend always adding this to every moment type. The preferred dimensions for Hero images is 1920x1080px and we support .jpeg, .jpg, .png files. If you are struggling to find images, please contact your account manager who can guide you to the Bright Image library where you will find a large collection of perfectly sized images for every place in the platform. 

11. Only add a Thumb image if your Moment will be pinned to the Home Page. You have ability to highlight Moments that you want to live on the Home Page. These moments might be exercises that you want learners to do over and over again. If you'd like your moment to live on the Home Page, be sure to toggle this option in the top right corner when creating the moment. 
Homepage Moment Toggle

Thumb images should be 800x1200 and we support the same file types as the Hero Image. The Bright team recommends leaving a 800x200 white space at the top of the image so that it fits nicely into the vignette that learners will see on the Home Page. Again, ask your account manager for assistance or direction to the Bright Image library if you'd like additional help here.

After you've completed the Moment set-up, click Save or Save +Close and now you can use the SCORM Moment just like any other Moment type in Bright.

Completing a SCORM Moment:
The default view when a learner begins a SCORM Moment will be launched in a current window in Bright just like other Moment Types (unless the SCORM file was published forcing it to be opened in a new window). There will be an option to expand to a fullscreen view and back to the standard view in the bottom right corner.

SCORM Pic 3

If a learner exits out from the SCORM Moment using the X in the top right prior to completing the moment, the learner's progress will be retained so that when the learner returns to the Moment they can resume from where they left off.

Depending on the success criteria of the SCORM file the learner must complete the requirements to pass the Moment prior to proceeding to the following assigned moments. This means if there is a quiz assigned with a required passing score, or if the learner is required to view all slides the learner will have to meet the requirements before proceeding in their assigned path. Once the SCORM file indicates that they have successfully completed the Moment, Bright will capture the "PASSED" status allowing them to continue.

Editing a SCORM Moment and Version History:
If you need to update or replace your SCORM content in existing SCORM Moments, you can click on the "edit" pencil icon from the Moment builder page (shown below on the left side above the SCORM file preview).

SCORM Pic 4

After clicking the "edit" button you will be able to view the SCORM Version History window. This will allow you to view a list of the current and all previous versions of SCORM files uploaded to the Moment ID. You will see the timestamp of when the upload took place, the version count and the user associated to each upload, the file name and have the ability to download the zip file of any SCORM files listed.

SCORM Pic 5

To upload a new replacement SCORM file, click on the "Upload New Version" option. This will allow you to either drag and drop or browse for the desired new SCORM file and click "Upload". From there you should receive a "Upload Successful" message that you can close and click "Save" to complete SCORM Moment update. 

SCORM Pic 6

When a new version is uploaded, all learners who haven't completed this moment will lose their progress and will be required to take the new version of this training.

SCORM Moment Reporting:
With this SCORM Moment release, reporting will look show the following statuses of the moment type: Not Started, In Progress, Completed, or Resubmit, as well as the number of attempts. Performance data from inside the file (e.g quiz scores, individual quiz question response tracking, etc.) is not currently supported in our reporting with this release.

The following reports will reflect the completion activity of learners assigned SCORM Moment types just like any other Moment types in Bright:

Activity Report
Moments Report
Team Progress Report
Outstanding Report
Assignment Report
SCORM Recommendations:
To help you prepare for this release, here are a few tips for smooth adoption of the new feature:

Inventory Your SCORM Files: Take some time to gather any SCORM packages that you may want to use in Bright. We know these often live in multiple places, so take time to hunt them down ahead of time.
Test All Files Before Delivering to Learners: One of the most common SCORM challenges through the last decade has been the wide variability of SCORM configuration and file versions. We've been testing SCORM 1.2, SCORM 2004 2nd, 3rd, and 4th Edition versions with great results. But due to the wide array of configuration settings, it's possible that there may still be edge cases that don't play as well for certain file types. Testing experiences before delivering them to learners is always a best practice, and will help avoid any unexpected challenges.
Plan + Communicate Changes to Assignment Paths: Since SCORM will live inside Moments, you'll be able to add SCORM-based experiences to any Assignment Path very easily. Give thought to where a unified learner experience could be useful in your programs. Keep in mind that adding these experiences to existing paths may impact completion % in reports, so give thought to whether it makes sense to communicate these sorts of changes to your teams ahead of time.
SCORM FAQ:
Q: What if we have SCORM files that are NOT in SCORM 1.2 or SCORM 2004 formats?

A: At this time we are not supporting any other formats, but please reach out to your CX Manager to indicate the additional SCORM file formats you would like to see supported so we can prioritize future enhancements to our SCORM Moment types.

Q: Does this mean Bright can now be used as an Authoring Tool to create SCORM files?

A: No, we still focus on providing the ability to create immersive experiences through practice based scenario in software and conversation simulations. We've simply enabled you to host SCORM files you've created in other authoring tools or purchased from other 3rd party providers on the Bright platform.

Q: Do you accept SCORM files from (insert Authoring Tool)?

A: As long as the Authoring Tool allows the file to be published based on our requirements, we should be compatible with the SCORM file.

Q: Will SCORM Moments be enabled for mobile devices?

A: This depends more on the authoring tool used to create the SCORM file, but as long as the authoring tool is optimized for mobile devices the SCORM files should be fully accessible when using Bright from the Google Chrome browser.




Testing a Moment
The best way to test a Moment is 'on the front' end. How can you do this?
How to Test a Moment

This guide will walk you through how to successfully test any Moment from the 'front end' so that you can experience it as a Learner would, and confirm that all validation is working as expected.

Go to admin.learnwithbright.com
1. Once you're ready to test your moment, click "Save + Close"
Save your changes by clicking this button. This will take you back to the Moments table in the Admin portal.

Once you're ready to test your moment, click 'Save + Close'
2. Click on the Hero Image for the Moment you want to test.
There will be a small Play icon on the image.

Click on the Hero Image for the Moment you want to test.
3. Click "Start moment"
Notice that this opened the Moment in a new tab on the "app.learnwithbright.com/" side of the platform. Initiate the moment by clicking the "Start Moment" button. Once you click this, you will see the Moment as learners will. This will let you test validation in canvases and preview thresholds and AI rules, too. 




Lesson Builder: Step-by-Step Walkthrough
Leverage our Lesson Builder to deliver courses and knowledge checks throughout your programs
The Lesson Builder provides Admins with the ability to create unique content and knowledge checks in seamless, compact courses. This article will outline the content blocks and step-by-step instructions on how to link your chapters together. 

Step 1: when you go to create a new Moment, you'll now see a new Moment Type listed: Lesson Builder. Name your new Moment, select the Lesson Builder Moment Type and click Save. Once saved you'll be able to click the Edit button below the moment type to enter the builder.



Step 2: once you click 'Edit' the builder tool will open and a pre-populated Chapter 1 will appear. To add chapters to your lesson click the "+ add a Chapter" button. Add a Knowledge Check or Quiz by clicking the "+ Add a Knowledge Check" button. As you do this, you will start to see different sections populate through your lesson. These sections can easily be reordered, by dragging and dropping them into different areas as needed. Note: you can also add chapter and knowledge checks by clicking the blue "+" icon in the middle of any section.



 

Step 3: once you are ready to add or edit content in your Chapter or Knowledge Check, click the pencil on the righthand side of the section. We will start by outlining the content blocks you can leverage for any of your chapter below. 

Once you're inside an individual chapter, you can write a Chapter description, and select one of the following content block.



Please see the purpose for each content block listed below. Further down in the article, we'll review how to leverage each content piece in detail. 

	Enter a text block to relay knowledge or information. Free form, long paragraph text with quill editor for text formatting.
	Drop images into your course to make the lesson more visually appealing. Can support .jpeg and .png files.
	Easily insert pre-recorded video content into your lesson. Supports .mp4, .avi and .wmv files types.
	Insert a process content type if you are trying to convey step-by-step directions that the learner must follow. 
	Create flashcards for testing and improving memory through practiced information retrieval. 
Enter Text and format text using different headings, fonts, and more!

Drop or upload an image to add visually appealing context to your lessons.  



Note: Images can be re-sized once loaded into your lesson.

Drop or upload an video file or insert a Vimeo or YouTube URL.



Show steps in a process that a learner must click through to reveal.



Quickly create the front and back of flashcards to test learner memory.



Step 4: Creating Knowledge Checks after any section is easy and a quick way to check engagement as your learners progress through your lesson. Once you've added the knowledge check from the main lesson menu, click the pencil icon on the left of that block. Once in the editor, you will see a similar layout as multi-choice moments. 



First, select the Question Order. This is to allow flexibility in the way the questions are served up to learners and deter users from sharing answers. Your options are:

All - Entered Order: all questions will be served up in the order you enter them in the system
All - Random Order: all questions will be served up in random order
80% - Random Order: eighty percent of the questions you enter into the system will be served up in random order
50% - Random Order: fifty percent of the questions you enter into the system will be served up in random order
Next, select a Threshold to Pass. This will serve as the score that learners must achieve on the knowledge check before they are able to move onto the next section. Think of this as the passing test score. You can choose any percentage from 50-100%.

Now you are ready to create your first questions. Click on the first question box to display the question settings. Then follow these steps:

Enter the Question Name: this is only for internal use/reference and will not be surfaced to the learner. Many customers will just use something like "Question 1" or a name that gives a little more detail about the question contents (ie. "Year Founded"). 
Enter the Question Prompt: this is the question or text that the learner will see. You can format the text using the quill editor. The ellipses will expand additional formatting options. 
Now navigate to the Add Choices section of the settings. You will add several choices using this box. There is no limit to the amount of choices you add but the standard is typically four options. 
Option Name: this is one of the options and the text that the learner will see.
Rationale: this is what the learner will see when they select this option. So if it's the wrong option, you might write something like "Not quite. We were founded a different year". You could also give much more specific feedback if a learner selects this option, especially if this is a common error learners make. 
Now click "Add Choice".
Start this process over again for each option you'd like to add to this question. 
Once you've added all options, navigate to the box just below the Question Prompt and select which option is the Correct Choice. 
Once the Correct Choice is selected, you can "+ Add a Question" as needed.
Step 5: Save your lesson! We recommend saving frequently as you are building your lessons, especially if they have a lot of content or you want to come back to your draft. Once you are finished with your lesson, be sure to Save. You can also preview the content directly from the authoring tool in the upper right corner. 

Still have questions about our Lesson Builder? See our Lesson Builder FAQ article for tips, tricks and more information on navigating the authoring tool.




Canvas Basics
The building block of a System Moment is a Canvas. Use Bright's layering tool to make screenshots of your system dynamic and test learners' navigation proficiency.
Creating a canvas is one of the more advanced Admin capabilities in the Bright platform. In this article we will give you step-by-step directions on how to add screenshots, create layers and set parameters so you can test learner proficiency. We highly recommend viewing the Bright Academy videos on Basic and Advanced Canvas building which you will find at the end of this article.

Before we begin, it is a best practice to create a baseline template for each system you are trying to prototype. There are many transactions that can happen in any given system, and if you have built one baseline canvas, it becomes very easy to clone a template and remove the steps or branches you don't need in order to train to a specific transaction. Start your first canvas by building out a strong template of the most commonly used transactions in that system and you will save yourself hours of repeat working time.

Now that you've scrubbed your screenshots of any data or PII, you are ready to load them into your canvas. Navigate to the Canvas menu in the Admin console and click the + sign. If you do not have clean screenshots, return to this article to learn more.

Next Name your canvas and give it a description. It's important to include the name of the system you are prototyping (ie. System - Master Template, System- Escalate a Case 1, System- Verifying PII, etc.). You can also tag your canvas to a specific team for additional organization, however this is optional. 

Next, you can drag and drop your screenshots into the Screens box on the righthand side of the page. As you learned in a previous article, your screenshots should be numbered so that they are added to the system in the correct order of operations. This is important because Bright will automatically link each screenshot to the next one. If the screenshots are out of order, you can simply drag them to the right location and the 'link or jump' to the next screen will update automatically. 
Once your screenshots are in the correct order, click Save to create the Canvas. 
Next click on the title of the first screen. This will bring you to the Canvas builder. In order to quickly navigate to any of the screens in the canvas, pop out the navigation panel using the arrows in the upper lefthand corner. This will show you all of the screens you added, similar to a PPT deck. You can move screens around, delete them or add new screens all from this navigation panel. You can also select the "Back to Canvas" option to return to the main page to edit or add Autofill Fields as you build.
You are now ready to begin placing layers on your screens. When you go to select a layer, it will always populate in the upper lefthand corner of the screen. Notice that all screens automatically come with the 'Jump' layer. This should be placed wherever the learner will have click in order to move to the next screen. Depending on what needs to happen on the screen will help you decide which layer types you need for your screen. Use the key below to see a description of all the layer types available in Bright.



There may be a variety of layer types that are needed on any given screen. The best example is if the learner needs to enter in new demographics in order to schedule an appointment or create a new lead in a CRM. You will likely need several Input layers to enter First Name, Last Name, Email. You may need to the Date layer to add the DOB, you may need the Text layer to add details about the campaign which is something that the system might automatically generate (not something the learner has to input). You may need to place a Toggle layer if you need them to confirm agreement and you may need a Select One layer to select their gender.
Now that you've decided which layers need to be on the screen, you can begin filling out the details for each of these layers. You click on the layer directly on the screen or select it from the list in the lower righthand corner. This part is extremely important. Everything you enter in the details panel, the learners will interact with. This is also where you can force a correct answer, enter Help details, change the color, font, weight of the text and turn on click guides. It's important to start from the top of the details panel for each layer, and fill out all of the necessary details. You can always Preview your work to see what it will look like on the frontend by clicking the 'Preview' button. 
Once you have placed all of the layers on your screen, remember to add the 'Jump' layer that automatically comes with every screen on the button or place where they need to click to move to the next screen. If any of the information that they entered on the screen does not match the correct answers that you've asked for, it will not allow them to proceed until they update that field. This is why including information in the Help Text is crucial. If the learner is tripped up, they can always press Help and learn exactly what they should enter into the different fields. 
Continue adding layers to each screen as necessary until your canvas is complete.
Additional Tips and Tricks
Cloning: In addition to adding layers, you may find that you need to clone a screen. You can do this quickly from the bottom of the page below the screen you want to clone. The cloned screen will appear directly after the screen you are on. You can then move it to the appropriate place in the flow. Be sure to keep the name of the clone different from the original, by default the system will add the word "Clone" to the end of the name.



Reuse Screen in another Canvas: You may find that you have a particular screen that you want to reuse within another Canvas. You can achieve this simply by knowing the Screen name or source ID and typing it in the Screens search bar. You can select your pre-existing screen and you'll receive a message "The screen is already in use by another Canvas. Do you want to clone it and add it to the current Canvas?". Click 'ACCEPT' and you will find the cloned screen at the bottom of the current list of screens. All the screen details will have carried over and you can now drag and drop the screen in your desired order of the screens.

Replace Background: you may realize that the UI has slightly changed in your system and the page where you need to enter in the lead details looks a little different now. In order to keep your training up to date, you can quickly replace the screen while preserving all of the layer details. Once you replace the background, you may need to move the layers to new areas on the screen but you will not have to recreate these from scratch. 

The Bright Academy Videos to walk you through canvases can be found in this simulation.

To continue to hone your Canvas skills, read our next article where you will learn about how to add layers to a library and leverage our Autofill feature to save time and create tight templates. 




System Moment Creation
Step-by-step instructions on how to create a system moment after you've completed your Canvas. A canvas alone cannot be placed in a simulation for learners, so you must set up a System moment. This is also a crucial step in testing your canvas.
First, name your Moment.

From the Moments menu in the Admin console, select the 'System' box as the Moment Type. 

Select the 'Associated Canvas'. This is where you pull in one of the canvases in your library that you want the learner to navigate. When you click on the box, the most recently edited canvas will populate but you can also begin searching the canvases by name if you don't see the one you are looking for. To learn more about how to create Canvases please see our articles about Canvas Building. 

Next, you must set thresholds to pass this moment. If the learner does not meet these thresholds, they will have to do the exercises again. This is where you can really start to measure proficiency and accuracy in your systems. 

Lifeline Count - Enter the number of times you'll allow the learner to ask for help and hints. If you decide you do not want to set a limit for this, we recommend putting in a high number like 99. You must enter a numeric value in this field in order to create this moment type. Once the learner has met your threshold, the help button will disappear.
Duration to Pass - This is different than the time that it takes to complete the moment. This is where you can actually put a time limit on the exercise and if they do not complete the transaction in this amount of time, we will serve the simulation up to them again. This field must be entered in time format: mm:ss. Again, if you don't want to set a limit, input a really long time like 99:99.
System Error Threshold number- This will measure how many times the learner has an error on screen when trying to move on. Similar to lifeline count, you must enter a numeric value and if you don't want to set a limit we recommend entering a high number like 99.


System Moment Thresholds

Next, enter the Duration. This is the time it will take the average learner to complete the exercise. For system moments, it's important to input the time you think it SHOULD take them to complete the transaction. Keep in mind, if you decide to overlay an audio recording over the system moment so that the learner has to navigate the system while listening to a live call, be sure to put the duration of the call recording as the Duration for this moment. Lastly, the Duration does not have to be listed in a particular format (ie. 00:00 or 5 Minutes). This is a text field so you can type whatever format makes sense. Just be sure to be consistent throughout the building process. 

Now enter the Short and Long description. The short description is mostly for the Admins and Coaches who will use this summary as a way to quickly see what the moment entails while building simulations or perusing the reports. The long description will be learner-facing prompt and should include the instructions about what the learner can expect during a system simulation. It's important to be descriptive in the Long description to give as much detail and direction to the learner.

The coach guidance is not necessary for a system moment since the platform will automatically be measuring proficiency and handle time according to the thresholds that you set above. 
Assign a Team Tag. This is not required however we do recommend tagging your content to the specific Admin team that this content is intended for. As you and your organization grow with the platform, this is the easiest way to organize content for your division, region, etc. To learn more about content tagging, see this related article.

Next, add a Hero Image. The hero image will be the backsplash image that is shown behind the Long Description when they first enter the simulation. If you do not add a Hero image, this will just be a blank, grey screen so we recommend always adding this to every moment type. The preferred dimensions for Hero images is 1920x1080px and we support .jpeg, .jpg, .png files. If you are struggling to find images, please contact your account manager who can guide you to the Bright Image library where you will find a large collection of perfectly sized images for every place in the platform. 

Only add a Thumb image if your Moment will be pinned to the Home Page. You have ability to highlight Moments that you want to live on the Home Page. These moments might be exercises that you want learners to do over and over again. If you'd like your moment to live on the Home Page, be sure to toggle this option in the top right corner when creating the moment. 
Homepage Moment Toggle
Thumb images should be 800x1200 and we support the same file types as the Hero Image. The Bright team recommends leaving a 800x200 white space at the top of the image so that it fits nicely into the vignette that learners will see on the Home Page. Again, ask your account manager for assistance or direction to the Bright Image library if you'd like additional help here. 
Next, click 'Save' to create the moment. A green confirmation box will appear in the lower left corner of the page to let you know that the moment was created successfully.
Once you are satisfied, you will need to tag the moment with Skills. Click the 'Skills' tab at the top.

Click Add a Skill / Rating.

Select from the Skills in your library. You can select multiple skills for an individual moment but note that system moments are pass/fail and coaches will not be rating them separately. You'll likely chose 1-2 skills and those skills should be aligned to the System Proficiency and the Transaction type. For example, one skill you might add for your system simulations is 'Salesforce Proficiency-Creating a Lead'. You can add multiple skills by repeating steps 15-16. If you do not see any skills, this means that you have not added any to your library. You will have to navigate to the Skills menu and add skills before being able to assign skills to this moment. See more information about this in our Skills Library article.

Once complete, click the Save + Close button to return to the Moments menu.
How to Add an Audio File to a System Moment
You may wish to have Learners listen to something while they navigate the System moment you just finished. This could be a tutorial, walking them through the system and their next steps, or a customer call they need to hear to navigate the system and practice the transaction. You can set this up in the Preview tab of a System Moment.

Note: you do not have to add audio to every system moment but it is a way to increase the challenge for the learner so that they have to listen to a call in order to obtain the correct information for the simulation.
Click on the 'Preview' tab. This is where you can overlay audio from a live call. Simply drag the audio file into the box or browse from your computer. It will take a few seconds to populate and then you will see the audio wave as a confirmation that it was uploaded successfully. 


From here you can listen to the audio, replace or delete the file. You also have the option to put in an 'Audio Break'. This means that you can automatically pause the audio at a certain timestamp if the learner has not reached a particular screen. It's a helpful way to ensure learners do not get left behind. It's also useful to input an initial audio break at 00:00 seconds and to chose the second screen in the canvas. There should always be instructions for the learner on the first screen in the canvas and you would likely want to give them time to read these instructions before the live call starts and they need to start entering information. 

To enter an Audio Break, be sure to select the screen that the learner has to get to before the audio will resume. Next, click 'Add Breakpoint' and Save. A blue bubble with the timestamp and name of the screen will appear. Click the blue bubble. This will pop open the details of the Audio Break.




What is a Canvas 'Variable?'
Variables let you pull in dynamic data sets, such as today's date and the learner name.
Ever been working on a simulation and wished you could tailor the details to match the learner or date? 

Enter 'Variables.' 

This is a growing list of data points that you can pull into Text or Paragraph fields that use dynamic data instead of static. 

To use them you simply type TWO curly brackets before and after the variable code. 

The variable options and code language are:

Learner Full Name:  {{learnerFullName}}
Learner First Name:   {{learnerFirstName}}
Learner Last Name:   {{learnerLastName}}
Today's Date:  {{today'sDate}}
For example, if you typed: 

Hi, {{learnerFullName}}- welcome to this simulation. 


During the simulation it would read: 

Hi, Grace James- welcome to this simulation. 




Smart Layers and Autofill
As you continue to hone your Canvas building capabilities, you will love to learn how to take advantage of some of the more advanced Canvas features.
Now that you've learned how to create a basic canvas, it's important to learn about some of our more advanced Canvas features that will allow you to add layers to libraries, leverage the autofill feature and create Smart layers that maintain a relationship throughout the canvas. In addition, we share best practices on how to put the finishing touches on your canvas before it's ready to be tested by your teams.

Layer Library
As you were creating your first canvas, you might have thought to yourself that it would be nice to save these layers in a folder somewhere to preserve the format, placement and details that you added to the layer so that you can quickly add it to other screens later in the flow, or in other canvases completely. Well you're in luck! Bright has developed a way to add layers to a library to save you time and organize your layers depending on which system you are prototyping. To add a layer to a folder, simply scroll to the bottom of the details panel and click the carrot next to 'Add to Library'. This will give you the option to add this layer to an existing folder or 'Layer Group' or create a new Layer Group from scratch.

Add to Library

This will preserve all of the layer's details including the format, as well as the placement on the screen. We recommend adding layers to Layer Groups based on the specific screen you are working with (not necessarily just the system in general). This will allow you to add that screen to any flow or canvas, click on the folder itself, and automatically place ALL of the layers on the screen in one click. While you can still choose individual layers, one at a time, the ability to place all layers at once is a HUGE time saver. See the video below for more details.

Layer Library Tips: 

When adding multiple layers to a library, add the lowest layer (furthest back) first, then add the layers in order towards the top layer. This way, when you click a library to add all layers, they will already be in the proper order.
Be sure to save the layer to the library without correct answers, click guides, or any specific information entered. This will save you time if/when you clone the canvas to create repetition. 
Smart Layers
Many systems have fields we enter information into and continue to see as we move forward in a given workflow. Smart Layers enable us to recreate this behavior. Any layer that requires learner interaction can become a smart layer (input, calendar, paragraph, select one, typeahead). 

Let's look at creating Smart Layers using a common use case. One of the most common smart layers you will likely use is a Typeahead layer used as a search bar in a system. Let's breakdown the best practice for creating a Smart Layer to recreate the search bar.

Click the Typeahead layer to add it to the screen of your canvas.
Adjust the location of the layer to the search bar of your scrubbed screenshot and name the layer, for this example "System- Search Bar".
Adjust the font style, weight, and size to match the system.
If applicable, add the placeholder text. Often this might be something along the lines of "Search" or "Name". Placeholder text is text that displays until the Learner begins inputing the information needed.
Be sure you do not have any correct answers or click guides enabled. If you do, the correct answer will be saved with the Smart Layer- from our experience this creates more work when cloning.
Use the Smart Layer Library option to save either into the general Layer Library or a specific layer folder. Use the carrot dropdown the same as regular layer library items to choose a specific folder or create a new one. Once you click either the button or a specific folder from the dropdown, the layer has been saved as a smart layer. Just like Autofill Layers, Smart Layers will show with a star icon to help identify these layer types.
Smart Layer Library
On each subsequent screen, navigate to the layer library to locate your new smart layer. Click the layer to add it to the new screens. Each screen with the same Smart Layer will contain the last information the Learner entered.
Layer Libraries
Go back and add correct answers and specifics as needed for the specific workflow you are building. Congrats on making your Smart Layer! 
Remember Smart Layers, and any other layers being added to a library should not include click guides or correct answers at the time they are saved to the Layer Library whether normally or as a Smart Layer. If these are enabled, you will end up using more time to go update the layer on each screen to remove the correct answer, update it to the new version, and remove the click guide- that is a lot of unnecessary time that we can avoid on the frontend!

Autofill Feature
Nearly every system has data that caries across multiple screens. Whether it is the customer's name, phone, or account number in a CRM, a case number in a ticketing system or any other data in your systems, Autofills will save you immense time not only when cloning, but when making your initial template/workflow.

Autofills are initially created on the main Canvas page. Below the title, you will see this option to add an autofill.

Click the Plus Icon to add an Autofill.
Autofill Add
After clicking the add option, you will be able to add a Label and Value. 
Autofill Added
Examples of some common autofills are pictured here.
Autofill Examples
**In this example, the system has the customer's full name in some places, but in others the first or last shows up individually.
After we have added the Autofill, we can go to the screen where the information appears. Now, add a text layer and right click (control+click on Mac) the field. This will display your Autofill options. You can also right-click the text layer icon initially to see the menu.
Autofill Right Click
If you attach the wrong Autofill, don't worry, you can simply click the X on the selected Autofill under the Layer Details to remove the autofill.
Autofill Layer Details
You can repeat this process for each autofill in each place it appears. The next time you clone the canvas, simply click the edit icon on the Autofill to update the details across all screens at once!

Advanced Autofill Tips: 

Dates inside your system can be set as Autofills- this helps if you care about your system simulations staying up to date. Simply edit the date as an autofill to bring the practice to the current timeline!

Grouping Autofills can help organize the layers. Hold shift then select multiple layers by clicking the blank space between the label and value. Then right click and select "Group".

Autofill Grouping 1

Now this becomes a dropdown group.

Autofill Grouping 2

Then when you add the autofill to a text box, it will look like this:

Grouped Autofills

This will be especially helpful as you dive down the rabbit hole of autofills to build clean templates that are quick and easy to clone!




All about Jumps
Jump Layers - and other layers that can jump - are crucial for canvas building. Let's take an in-depth look at configuring our Jump layers.
Jump layers, as the name implies, jump us from one screen to another. These will be one of the most common layers in your canvases- in fact, when you first upload your screens, you will notice that the system automatically puts a jump layer on each screen. We have several configurations you can use to get the most out of your system simulations, so let's jump in.

Jump to Layers
We kept the most important option first. In the jump to field, you will see a dropdown with each screen of the canvas. This allows you to select which screen should appear when a learner engages this jump layer. 

By default when you upload your screens, each jump layer will be set to 'automatic jump' to the next screen in order. If you need to skip one or more screens, just change the selected screen on the dropdown.

Checkbox Options
These checkbox options relate to the behaviors of the screen we are jumping to next.


Scroll to Top after Jump- If the screen is a scrolling screen, this option would determine if the next screen will display at the top, or if the scrolling will stay as is. If we had scrolled 2/3 down a page then click the jump layer and we:

Check the "Scroll to Top" box, when we jump, the learner will be at the top of the screen.

Leave "Scroll to Top" unchecked, when we jump, the learner will still be 2/3 of the way down the page.

Automatic Link- The default is for a jump layer to go to the next screen in the canvas, so this box will be checked.  This means that the screen will jump to whatever screen is next, even if you re-order your screens.

Caution - Unchecking the automatic link will mean that no matter what edits you make to the order of my screens, the jump layer will always jump to the screen you selected from the dropdown (and this option won't change unless you manually update it). Use this when you want to skip screens and land somewhere other than the next screen.

Hide Layer from Screen- if you check this box, the jump layer will be hidden from view, meaning that you will be unable to click on the layer. You should use this only when the learner should use a hotkey to jump to the next screen. Select the appropriate hotkey or key combination and then select "Hide layer from Screen". Now the learner will have to use the hotkeys to perform the navigation instead of looking for the hotspot in some obscure place on or off the screen. It will also ensure the jump layer is not highlighted when the learner clicks on the Help icon for more information.

Highlight First Layer- Selecting this option will automatically put the cursor in the topmost typing-field (input, paragraph, typeahead, etc.) in your Layers list in the canvas. This feature will be off by default.

Tip - We recommend only using this setting when dealing with a keyboard-based or DOS system that does not allow a user to click on fields.

Adding Hotkeys to a Jump Layer
Many softwares have hotkeys enabled to speed up the process. For example, can you hit Enter or F2 to move forward in your software? Those are hotkeys! Bright can recreate these experiences for learners to build the best habits before ever touching the production instance of the software.

Before we talk about how to add in Hotkeys, we'd like to share a bit of background. Bright is web based, and there are certain hotkey combinations that can not be overwritten in a web browser. Some examples are: Control+N (opens a new window in your browser) and Control+S (saves the current page to your files). If your system uses one of these, we recommend omitting it on that particular jump, but you can still call it out in some help text.
Screenshot 2024-02-13 at 4.01.38 PM

Main Key - This is the section to use if you need to add a single key as a jump. Think Enter, F2 or even T. If the combination is a single key, the Combo Key field above Main Key will be left blank.

Combo Key - This is a secondary key required for some hotkey combinations, such as Shift + [key] or Control + [key]. In the example of Shift + P, Shift would be your combo key and P would be your main key. 

Tip - If you ever have a system that requires a key combination to continue rather than a click, you can shrink the jump layer and move it off the right edge of the screen. As long as the hotkeys have been set up, the learner will jump when they use the key combination.

Using tab to quickly move through input fields in a system is common, but this is another default behavior in a web browser so you don't need to build that out as a Jump layer. Simply drag your layers into the order you want to tab through with the first layer being the top of the Layer list. By default, when you press your tab key, the cursor jumps to the next learner typing-field (input, paragraph, or typeahead).

Other Layer Types that Can Jump
There are two other layer types that can have a Jump added to them, if the learner enters or selects the correct answer. 

Typeahead Layers
Screenshot 2024-06-10 at 12.54.29 PMWhen using a Typeahead layer, you can set up a Jump to another screen based on whether the Learner selects a Correct Value. The learner would 1) begin typing, 2) select an Option from the menu that appears below the text input field, 3) [if a Jump is set up] select one of the Correct Values entered and then Jump to the screen set up.

Select One Layers
Screenshot 2024-06-10 at 12.54.40 PMWhen using a Select One layer, you can set up a Jump to another screen based on whether the Learner selects the Correction option from a drop-down menu. The learner would 1) select the Correct Value (there can only be one) and then 2) [if a Jump is set up]  Jump to the screen set up.

Help Functions for Jump Layers
Help functions are available for all layer types that require learner interaction (input, paragraph, date, select one, typeahead, toggle image, jump) and are designed to help give learners a clear path forward if they get stuck.
Screenshot 2024-02-13 at 3.37.11 PM

Help Text - Help text is what the learner will see if they click the blue "i" icon during the system simulation. If your Help Text is too vague, a learner can get stuck and frustrated without a clear path forward. Good Help Text might read: "Remember to click the client's name to open their account." Help text should always be included on each respective layer the learner needs to interact with. 

Click Guide - These are the blinking red beacons that help a learner's attention be drawn to a specific field on the page. When you have this option turned on, you will also have the option to enter text; this text will be visible when the learner has their mouse hovering over the click guide. We recommend using these in moderation. Too many click guides (or using them on all of your system simulations), means that learners aren't tested on their ability to navigate without assistance before they begin their role.

Best Practices + Tips
When building, always test your canvases on app.learnwithbright.com to see the learner experience and verify the jump is landing and behaving as you intended.

Be sure to verify jumps if you clone or add screens.

By default, if the automatic link is checked, the jump will automatically update where it jumps to when you change the order of screens (including cloning or adding screens).

You can use multiple jump layers on one screen- maybe on a home page of your system, there is a toolbar at the top and another on the side. If clicking both places in your system gets the user to the same place, you can put 2 jump layers with the same setup (jump to, and checkbox options).

Don't overdo it... while you can put 8 jumps on the same screen to go to any number of places within the software, all of your system simulation moments are most effective when they are built to practice a single workflow. That means if the learner needs to make a case and mark it as resolved, I do not need a jump on the escalation process.

Keep Help Text vague. Rather than, "Click John's name to open his account" use, "Click the client's name to open their account." Using vague (but still specific) feedback helps when you will be cloning the canvas to make additional practices for your learners.




How to Write Effective Help Text
What types of layers need help text? How do I write highly effective help text?
Layers Needing Help Text
Help text and click guides are a great tool to help learners progress in the system simulations. We are recreating real-life moments in Bright, using the software they will use in production. But, in Bright, when a learner is stuck, they do not need to raise their hand or frantically message their supervisor, we can guide them when they click the help icon.

Help Icon System Sim

Anytime a learner will interact with a layer, we should add help text. Examples of learner interaction include: 

typing an email into a search bar (typeahead)
entering the subject line of a case (input)
toggling a checkbox in the system (toggle image)
clicking a button to get to the next screen (jump)
Multiple layers can require interaction from a learner before proceeding. Each layer that has interaction needs help text since any of these pieces of interaction can be the element the learner is stuck on. Likewise, we do not need help text if the learner will not be required to complete a field, maybe we have an optional field (input) that has no correct answers- if there are no correct answers, the help text will not appear. So how do we write effective help text?

Tips for Effective Help Text
While we want to be specific, we still should aim to make our own lives easier. One of Bright's pillars you have heard time and time again is spaced repetition. A System Simulation is significantly more valuable when the learner experiences the transaction multiple times in the system. We recommend keeping these tips in mind each time you look to enter your help text.

Screenshot 2024-02-13 at 3.37.11 PM

Remember your Audience - the help text is designed for learners who get stuck. Always think about yourself in their shoes. Would your help text be useful if your mind blanked when you got to this screen? Is your help text too vague that someone who is stuck will be more confused?

Be Specific - going hand in hand with your audience, you need to keep your help text specific. Do say things like, "Remember to click the customer's name to access their profile." Avoid saying things that are too vague- if a learner is stuck, help text stating, "Open the customer's profile" can leave them with more frustration.

Maintain Scalability - make our canvas scalable by refraining from getting too granular with our help text. Using the above example, "Remember to click the customer's name to access their profile." is a great piece of help text as it helps the learner identify where to interact on the page to move forward, AND it says "customer's" instead of "Annette's" name. If we were specific to the scenario and used Annette, each time we clone this canvas to make an additional practice of the same call flow, we would have to update each piece of help text. Scalability definitely plays into the nature of effective help text- in this way the effective pertains to us as an admin. 

It may seem that the number of layers requiring help text on a screen is high at times. However, learners often retain information from previous sessions. Our platform dynamically helps learners by removing unnecessary help text related to fields they've already completed correctly. Test this by setting up layers with help text, then as a learner, observe how completed fields disappear from the help text after correct entries.

These tips are your key to crafting guidance that truly resonates with your learners. As you embark on the journey of effective help text, remember the power of spaced repetition and the dynamic nature of our platform. The ultimate goal is to assist learners and streamline the cloning process, ensuring a seamless and scalable learning experience. Dive in, make those help texts shine, and witness as Bright learners evolve into true masters of your software systems. Happy guiding!




Finding Balance in Click Guides and On-Screen Help Text
Navigating the fine line between support and dependency in system simulations
Ever heard the saying, "Too much of a good thing can be bad"? Well, that applies to helping learners in the world of system simulations. Imagine every canvas filled with click guides and on-screen help text, sounds overwhelming, right? We don't want learners mindlessly clicking red beacons or becoming overly reliant on on-screen prompts. So, how do we strike the right balance? Let's dive into the best practices for click guides and help text. 

Scale Back Click Guides Rapidly
Click guides are handy, but too many can lead learners down the wrong path of dependency. The golden rule here is the "One + Done" philosophy. When learners encounter a field or button for the first time, introduce a click guide. After that initial interaction, turn off the guide. This practice ensures learners focus on "what" to do rather than "where" to click.

Flexibility is key. While "One + Done" is the starting point, your learners may benefit from an extra guide or a mix of on-screen help text with images in place of a click guide. Monitor effectiveness and adjust as needed, keeping your learners in mind.

Scale Back Help Text Gradually
Spaced repetition is our go-to pillar for success. Three reps on a workflow lay a strong foundation, but more practice enhances learning. Let's breakdown a structured approach to on-screen help text over three moments in a workflow.

If we start by looking at a structure of three reps on one workflow, we can build a helpful pattern of how our help text is implemented. Now, we need to establish that in this context, we are specifically talking about on-screen help text. For more information on the help icon help text, check out this article. But let's take a look at putting together the idea of how a screen looks with click guides and help text on each of the three practices we have for our learners.

1st Moment  Intro to the Process

Display in-depth help text and click guides to guide learners through the initial exposure.

On-Screen Help Text

2nd Moment  Practice with the Process

As learners get familiar, remove the click guide (One + Done) and gradually scale back the on-screen help text.

On-Screen Help Text Part 2

3rd Moment  More Practice Handling the Process

By the third run, mimic the production environment. If a learner gets stuck, they can use the blue "i" to access help text on the jump layer.

On-Screen Help Text Part 3

In all three cases, the help text accessed through the blue "i" remains constant, providing consistent support. The goal is to reduce on-screen guidance gradually, ensuring learners are well-prepared for the production environment of your software. These practices strike the right balance, helping learners thrive without feeling overwhelmed.




Creating a Simulation
Stitch moments together to create a dynamic simulation
The first step to creating a Simulation is to remember that Simulations are made up of multiple Moments. Before you create a new simulation, consider what Moments you'll need, their order, and the overall simulation strategy. 

When you're ready, visit the 'Simulations' menu in the Admin Portal, click the + sign in the upper left hand corner of the table and follow the steps below which will walk you through naming the sim, creating Moment Groups, and associating Moments and Resources to Moment Groups.

Click to + sign to create a new simulation.

Name your Simulation in the Simulation Details tab.

Enter the Duration. This should be the sum of all Moment durations. Remember that this is text field and does not require a particular format (ie. 00:00 or 5 Minutes) but we do recommend consistency throughout your programs. The Duration will show up in the carousel, as well as the simulation box on the homepage. See the example below.


Enter a short and long description. For Simulations, the short description will show up for the learners in My Path as well as in the carousel on the homepage. The Long Description will show up on the Simulation landing page when they click on the simulation box itself. See these examples below:
Short Description in Carousel on Homepage
 


Short Description in My Path
 

Long Description on Simulation Landing Page


Next assign a Team Tag. This is not required however we do recommend tagging your content to the specific Admin team that this content is intended for. As you and your organization grow with the platform, this is the easiest way to organize content for your division, region, etc. To learn more about content tagging, see this related article.
Next, add a Hero Image. The hero image will be the backsplash image that is shown on the Simulation landing page. If you do not add a Hero image, this will just be a blank, grey screen so we recommend adding this to every simulation. The preferred dimensions for Hero images is 3840x1700px and we support .jpeg, .jpg, .png files. If you are struggling to find images, please contact your account manager who can guide you to the Bright Image library where you will find a large collection of perfectly sized images for every place in the platform. 

You must add a Thumbnail image to Simulations as well. This will be the image that is depicted in the carousel on the homepage. If you leave this blank, the carousel will just show a white box in the place where there should be a picture.
Simulation Thumb images are a different size than Moment Thumb images. They should be 1400x700 and we support the same file types as the Hero Image. Again, ask your account manager for assistance or direction to the Bright Image library if you'd like additional help here. 
Once the details of the Simulation are complete, click 'Save' to create the Simulation. A green confirmation box will appear in the lower lefthand corner to let you know that you've successfully created the Simulation.
Next you will need to add at least one Moment Group. Moment Groups define what 'stages' appear on the Simulation landing page.



To create a Moment Group simply click the blue 'Add Moment Group' button and enter the name of the Moment Group. Each time you create a new Moment Group, a new tab will appear on the Simulation where you can further define the group and associate Moments and Resources. Moment Group order can be adjusted through Bright's simple drag and drop feature. If you need to edit the name of the Moment group, click on the pencil on the righthand side of the moment group box. See a short demo below.


Next, you'll want to add a detailed description to each of your Moment Groups. Select the tab at the top for the first Moment Group that you've created and write a detailed description. Your learners will see this as a pop-up before they begin the simulation so be sure to include as much detail as necessary to set the tone for the exercises in this stage of the simulation.


Now it's time to add moments and resources to each of the Moment Groups. Click on the 'Moments' box and the most recently edited moments will populate to the top of the menu. You can also search for moments by name or unique ID.
Continue selecting moments until you've added all of the moments you'd like to see in this stage of the Simulation. The learners will complete each moment in the same order that you list them in the Moment Group. To change the order, simply use Bright's drag and drop feature.
You can also remove moments from the Moment Group by clicking on the trashcan or the 'X' in the search bar.

In addition, if one of your moments is missing media or a skills tag, you will see a the following warning signs. 

You can update these details directly from the Simulation details page by clicking on the Moment. A modal will popup and you can edit the moment on the spot. Once you're done editing, click Save and continue crafting your simulation. Watch a quick tutorial below.


You can add resources to the Simulation as well. Scroll down to the Resource section of the simulation and populate resources in the same way that you populated Moments. When a learner is on the Simulation landing page, these will appear pinned to each Moment Group. 

Admin View

Learner View

Once you have finished creating all of your simulations it's time to put them into an Assignment Path. 




Creating and Editing Assignment Paths
Stitch together simulations to create a dynamic Assignment Path that is ready to assign to learners
Once you've created all of your simulations, it's time to put them together into a learning journey, or Assignment Path. This is the final step before you begin testing the content and delivering an experience to your learners. 

Navigate to the Assignments menu in the Admin console. Note: this menu is only visible to Super Admins so if you do not see this menu, speak with a Super Admin in your organization or reach out to your account manager at Bright. 
Next, click the + sign to create a new Assignment Path

Use the first field to name your new Assignment Path. This is your chance to be creative and/or use a naming convention that clearly defines who this learning journey is aimed at. 
Assignment Path Name
Next you will be able to add Team Tags to the assignment path. Remember, just like moments and other content, team tagging will include selecting a Tier 1 team.
Assignment Team Tags
The next option is the Description that is visible to learners. This is only used if the assignment path you are creating is featured in the Explore Tab. Think of this as the tagline for your assignment.
Assignment Description
If this assignment path will be used in the explore tab, the image will serve a similar purpose to the Simulation Thumb. For these images, however, you can add your 1920x1080 image and the platform will automatically resize the image to fit the explore thumbnail. 
Assignment Path Image
The next set of options all pertain to the explore path. For more information on explore and these specific options, check out this article in our help center. If you do not want learners on the platform to be able to add or remove this assignment path- say for your new hire assignment path, uncheck the Explore Path Searchable option. This will set the behavior for the path to only be accessed when an admin assigns the path in the user panel of the admin console.
Explore Path Options
Another addition in 2024 is the ability to assign badges to an assignment path. This feature allows a learner to earn a badge once they finish the assignment path. More information about our thoughts on effective badging can be found on this help article. If you want a badge associated with your assignment path, you will first need to create the badge. Then select it from this dropdown.
Assignment Badge Dropdown
The last piece you will see is the Path Details. This is a list of the content that resides in your platform. Use the Moments, Simulation and Resources radio buttons to decide what will show up on the homepage for the learners. We recommend starting with the Simulations that you want to populate in the homepage carousel. Once you toggle the Simulations radio button, you can either search for simulations by name or scroll through the list below. Once you find the simulation you are looking to add, simply click on the box and it will start to populate in the righthand column. Drag simulations up and down to change the order or use the trash can to remove the simulation from this learning journey. Don't worry, you are not deleting the simulation! Only removing what the learners will see in their Assignment Path. Be sure to Save after adding the appropriate content!

You can also pin individual moments to the Homepage. These might be exercises that you want learners to be able to revisit with ease. We also recommend pinning moments like the Platform Welcome Tour and maybe a Moment related to your organization's Mission, Vision and Values. These show up below the homepage carousel as cards along the bottom. You can do the same with important Resources, which will show up in the final section of the homepage. 

Lastly, you can edit existing Assignments from the Assignments menu as well. Simply click the pencil to edit the simulations, moments or resources that reside within that path or update the name accordingly. You can also click on any of the Simulation titles to update that Simulation on the fly. Important Note: if you make updates to any of the Simulations, Moments or Resources in one assignment path, that update will carry through wherever that Simulation, Moment or Resource is being used elsewhere.




The Skills Library
Every Moment will be tagged with a skill and will help you build insights into skill proficiency and gaps for individual learners
 


At Bright we strongly believe that being able to leverage individual skills data will create a future where you have insight into how someone will behave in their role PRIOR to them hitting the floor. Predictive skills data is an emerging space and Bright is a leader in delivering transparent data to our customers that help them make informed business decisions ahead of time. We believe that the way learners show up in practice and training will predict what kind of worker they will be on the floor. A key piece to this powerful tool is creating a Skills Library that aligns with your quality standards and to the practice and activities you map them to in the platform. Your Bright account manager will work with you to brainstorm this customizable library for you and make suggestions based on the data you are interested in and the business objectives you are driving towards. 

To initially create the Skills Library, navigate to the Skills menu in the Admin console. Parent skills are created by clicking the + sign and typing in the Parent Skill (ie. Empathy, Communication, Systems, Product Knowledge). Once the parent skill is created you can add more detailed, individual child skills underneath them (ie. Tone of Voice, Confidence, Handling Objections, Salesforce, Product Line). 



Best practice is to keep your parent skills to 4-6 per learning journey. Be mindful these parent skills should be broad enough to apply to a variety of use cases.

Once your skills library is complete, you can begin tagging one skill or multiple skills to any moment. The only exception is Multiple Choice moments. Multiple Choice is typically used to do knowledge checks and therefore if someone completes these moments successfully we do not believe they have necessarily accomplished or obtained any skills other than memorization. All other moments can and should be tagged with skills based on what you've asked the learner to do in that particular exercise.



These become particularly important when you are writing AI rules and when you are drilling down into the Reporting suite for an individual. Drill down into the Team Progress Report and click on a learner's name to see the skills profile. Notice the summary is broken apart by the parent skill. Keep that in mind as you design your skills matrix!




An Intro to the Coach Inbox
A quick guide to using the Coach Inbox to drive learner performance + preparedness.
One of the most important features in the Bright simulation platform is a Coach Inbox where designated users can review learner Moment submissions, provide feedback, and rate them on a scale of 1-4 stars. This information drives our skills-based reporting and helps serve content back up to learners who need more practice.

Navigating to Your Inbox
If you are a designated Bright Coach, when you click on your Alerts Button in the upper right hand of the menu bar, it will take you to your Inbox.
Notification Icon
*Note: If your alert bell icon has more items than your inbox, don't forget that you are also a learner and may have items to review in the Feedback + Practice section of your My Path page.
What's In Your Inbox?
It's important to note that there are a few types of moments that will not show up in your inbox for your team! The following types of moments skip your inbox and provide immediate feedback to learners:
Writing or Audio recordings with AI Coaching that is Live
System moments
Multiple choice moments

Filters + Ratings
Once you land in your inbox, there are a few things to note: 
Your inbox is automatically filtered to show only unrated moments. You can toggle this to see moments that have been rated.
You can filter your inbox by moment name, learner name, team or even by submission dates.
When you click on a Moment, you can leave comments + select a  Star Rating
You can leave audio, video, or written comments and even upload a helpful document when providing feedback on a submission.
Coach Guidance is created by your organization's training team - consider any content here as you determine ratings and feedback
A popup will confirm your rating went through!




Introduction to Bright's AI Coach
If you've seen Bright's coach inbox, you know it's a wonderful way to engage your team and provide personalized feedback that helps them grow faster.


But we're also keenly aware that it's more difficult to SCALE that kind of personalized feedback. That's why one of the features we're proudest of is Bright's AI Coach. This brings natural language processing - or NLP - into the platform to review what learners write or say, and automatically coach their submissions. 


We believe this is part of the future of learning - the ability to wield practice and personalize coaching at scale.


We're going to walk you through this capability step by step. Before we do that, I want to help frame your expectations on this. Because our AI Coach is a very substantial feature set. NLP Rules builder and AI Coach expert are the kind of thing you can eventually put on your resume. But it's an emerging technology, so something you'll want to take the time to really master. 


To start, let's look at the high level process for getting your AI Coach to work in Bright. 


The feature set has 5 parts: 

AI Rules- These are the rules we create on the Skills + AI tab of a moment and the node of a Conversation Builder. We recommend getting comfortable with standard moment AI rules before jumping into a Conversation Builder.
Speech to Text- This is how audio moments are able to be graded by AI. Speech to Text takes the Learner's audio recording and changes it to text just as Siri and other programs can take the audio to create messages and type notes.
Coach Inbox Previews- We want our AI feature set to be scalable, but we also need to ensure a high quality. We use a Training Mode with new AI rules to allow a human coach to verify the AI rating in the coach inbox before serving the score + feedback to Learners. 
AI Reports- The AI Training Report allows a quick view of all AI rated moments and whether they are still in Training or Live. You will also be able to see the acceptance rate and the total submissions run against the rule. This is the best way to determine when we are ready for the rule to go Live for Learners.
AI Coach Report- Even after turning an AI rule live, the Bright AI in the Coach Report will show the total number of ratings done by the AI Coach. We can dive deeper into this report to view specific submissions to spot check our rules as needed to maintain a high quality across our Simulations.

And these features match the flow for how to activate the AI Coach. As mentioned previously, we recommend starting with moment AI coaching before building your first Conversation Builder. Let's take a look at how we begin to scale our training by implementing our AI Coach.

Create either an Audio or Written Moment type. For more information on how to make these, check out this link for creating an Audio Moment and this link for creating a Written Moment.
Navigate to the Skills + AI tab. 
All AI coaching is tied to Skills from your Skill Library. After Selecting a Skill, you will see the option to Add a Rating Rule. Click this option.
Add a Rating Rule
Now comes decision time. We have several options to tailor each AI rule to the moment being coached. 
Number of Attempts before human coaching (1-4). This is how many times a Learner would submit against the AI rule without meeting expectations (3 stars) before the submission is sent to the Coach Inbox. The goal here is to avoid a Learner getting stuck in a loop, unable to progress. If the Learner is struggling this much, this gives a human coach the opportunity to see and intervene with the Learner.
If Statement- In most Cases we will use the Learner Submission option. This option will setup the AI to grade the content of the Learner's submission. In some cases, you may want to use duration to promote a learner being concise or more in depth. Be mindful the duration should only be used when the moment type is set to Audio. If the duration was selected, you will see the next dropdown become options for less than or greater than with the option to enter a value in seconds.
Full Intent Match - We recommend this as your first choice for AI rule building. This match will allow variations in verbiage from a learner while still meeting the requirements you define, providing the best learner experience. This is our newest option, so more information can be found in this article.  While other options are present, this truly covers any use case you may have.
Phrase Match - This is the only other match type we recommend using. Phrase Match is inherently more strict than Full Intent, which can lead to learners feeling like the AI is too picky. You will add specific words or phrases, the AI will look for these in the Learner's submission. It will accept different tenses, and filler words between the words of your correct phrase. If your Phrase Match term was "have great day", the learner would get credit for "Have a really great day" since the keywords are present. For more information, check out this article dedicated to Phrase Match.

Remember Phrase Match only requires ONE term to be included in the submission in order to pass that rule.

Example - If you have "email" and "address" as separate terms, a learner submission of, "This has been addressed" would pass as address matches one term.

It will take a little getting used to, but you should definitely play around with the submissions to start to understand how the AI is working. 
As you have likely come to learn, we strongly encourage meaningful feedback. The AI feedback allows feedback tailored to each element being graded. So far we have only made 1, so you will see a green box for what learners see if they followed the rule and an orange box for when the missed the rule. It is important to provide direct, meaningful help without giving the answer. For example the followed the rule feedback might say, "Our call flow always ends with a positive tone when we wish the customer well." Meanwhile, the missed rule feedback might look like this, "Refer to our call flow diagram to be sure you include the 3 pieces of a call wrap." This specific feedback gives the Learner specific instructions on where to find the information to review before attempting the moment again.
Add Stars for the Learner following the AI rule. In total, at least 3 stars are required to save.
Once you Save, the rule by default starts In Training. We will discuss this later in this conversation. For now, let's look at some additional options that are present for our AI Coach.
Now this is a good place to talk about Transcription in the platform, which is a type of speech to text. The speech to text by nature is set to strip punctuation for speech in an effort to simplify the overall process. You probably noticed this if you have tried to put punctuation within your AI rule terms on an Audio moment, since the system will not let you enter those special characters. It is important to note that this does mean it takes more time for longer transcriptions. But the overall time saved from the AI coach compared to human coaching will make up for it many times over!

Connectors
Connectors enable additional criteria for the same Skill/Star rating when using Phrase Match. Whether we are using the AND connector to look for the learner to mention both of the options we have for the caller's concern, the OR connector to allow a learner to use one of the required pieces of caller verification, or the THEN connector to check for a process being communicated in the proper order, they are all available for your customization.

Best Practices with Connectors:

We should only use 1 intent algorithm per moment. Since the Full Intent AI is looking for a concept rather than specific verbiage, we recommend including all parts you are looking for Learners to include in the Intent Samples. This will result in the best experience for you and your Learners.
Only 1 type of connector can be used, though multiple can exist under the same Skill. For example we could use 2 AND connectors to look for the Learner to mention 3 separate pieces of information, but we cannot use an AND plus an OR connector under the same Skill.
Simply use Full Intent AI (without connectors) to look for a concept being communicated with specific compliance verbiage as well. Maybe you want your Learner to provide empathy while stating a specific name of a program or policy available for the situation or you want them to educate the caller on the next steps of a process while using the caller's name. 
Common Errors
Common Errors are an additional feature within the Bright AI Coach. These allow us to identify the common errors existing employees struggle with and call them out specifically. The Common Errors function the same as adding terms to your existing Phrase Match rule. If a Learner would otherwise pass an AI rule, but they hit a phrase or keyword in the Common Errors, they will see the Common Error Feedback (our grey feedback box). This will also mean the moment is served up to the Learner again to give the opportunity to meet all the desired behaviors without our common errors.

An example might look like this:



Here you can see we want our Learner to ask for the reason for the call. But we have heard some cases where our existing employees have used the, I know you will cringe too, "What's your issue?" verbiage. Yikes! 

However, now during the training process we can aim to curb the behavior and reinforce better phrasing. In the image, you can see our rule would give 3 stars for any of these types of phrases:

How may I assist you today?
What can I do for you?
What can I help you with?
What questions do you have today?
What is the reason for your call today?
Meanwhile the Learners who use our problematic phrasing will see the feedback from the grey box and be asked to resubmit this moment. This is the concept behind common errors.

Training the AI Coach
In a perfect world, we would make an AI rule and it would be flawless, ready for Learners. Unfortunately, this technology is new and there is a lot of variables in play. Each of us have a diverse group of Learners which at a basic level means we cannot predict every possible verbiage we would accept on the first iteration of our rules. This is why the AI Coach starts In Training for each new rule we create. 

The process of training the AI at a high level is:

Create the first iteration of the rule
Test the rule using the test a submission feature below the AI rule builder
Ideally you are using intent, so keep an eye out for the rationale of the AI grader. You may find the grade is correct but the logic is wrong.
Make edits and save/republish your second version
Have at least 10-15 Peers and/or Learners go through the moment
Make needed edits to the AI rule.
Once the Acceptance rate is 90%+, with at least 10 ideally more towards 15 submissions, turn the AI rule Live.
Continue to listen to Learner feedback and spot-check the AI Coach using the reporting suite we mentioned at the start of this article.
We have already run through how to make the first iteration so let's pickup the training process with number 2. For this step in the process, we need to add our moment to an assignment path and have 10-15 Learners go through the moment to answer how they would. These Learners should typically be a core team such as your trainers, QA, or other leaders within the L&D umbrella. Including QA is one of the best practices to ensure each AI rule is up to standards from the team who knows those standards inside and out.

Before we get into the process, you may feel that the AI rule you are making is simple or maybe we are on a time crunch to push it live, but it is important to NEVER push an AI rule live without putting it through training. Training the AI is one of the processes that, when done, creates a smooth experience for your Learners. 

The next phase of the process is to train and edit your AI rules based on the submissions you're receiving through the Coach Inbox preview. This is SO important that we have an entire lesson to go over best practices for this in detail. For the purposes of this lesson, let's just leave it at you're going to make sure your AI rules are working BEFORE you make them live. 

So now comes the edits. In our Coach Inbox, we can view the AI In Training ratings for our Learner submissions. This shows the AI grader suggestion. We can playback the audio sample or even correct the transcription if needed. But our main goal is to verify the AI grading matches what we would rate the submission. 

Coach Inbox Training 1

In this image, we might have found that asking, "How can I be of service?" is acceptable for probing the reason for a call. If we want this to be correct, we can click the Edit AI Rule option. This will open the moment in the admin console where we could edit intent to allow this specific verbiage or add "service" to our terms if using Phrase Match. If the rating was accurate, say we decide this is too informal for our brand, we would not need to make an edit to the AI rule for this submission. As a best practice, we recommend viewing the other sample submissions to determine all edits needed to make them all at once. Then, we can save the moment and return to our coach inbox. In the upper right, we can refresh the AI rule to re-grade the submission with the edits we made.

Once we are in agreement with the AI Grader suggested ratings, we can simply scroll down and select Save & Archive. This sends the feedback to the Learner and removes the submission from the Coach Inbox.

Now for the acceptance rates and turning the AI live. For this, we turn to the reporting suite. Under the Reporting tab, you can navigate to the AI Training Report. This report is meant for this process. Here you can see how many submissions to each AI rule have been graded. Remember to look at the Total Training Interactions and Acceptance Rate. Our goal is at least 15 and 90%+ respectively. Once you feel confident the moment is ready for live learners, you can toggle the AI rules live from this same page OR directly in the moment. 




What is Intent AI and How do I Use It?
Bright utilizes large language models (LLMs) to analyze learner statements to see whether they match the tone, meaning, and substance of your standards.
The 3 statements below are excerpts from a software sales manager's discussion with a prospect. Read each statement, then analyze what you think the common thread is across the 3 statements.

Thanks for letting me know that you already have a CRM provider in place. Have you ever considered getting a price quote from an alternative provider?
I'm glad you're happy with Salesforce. For what it's worth, we've had quite a few companies similar to yours switch from Salesforce in the last few months. Do you have a minute to hear why they made that choice?
Thanks for your willingness to share a little about your current platform. I hope you don't mind me asking, but how is your current platform using generative AI? I ask because this feature has saved our customers millions this year compared to platforms that don't have this feature.
In each of these statements the sales manager: 

Is trying to overcome an objection from the prospect
Is using open-ended questions
Is first responding kindly to the objection before making the attempt
Each statement uses VERY different words and language, but they all have a similar intent. And it just so happens that each are correctly applying the training they received from their company. 

With this in mind, let's talk about Bright Intent AI. 

How it Works
Elsewhere we've outlined how Term and Phrase AI matching works. The concept there is simple: we look for core phrases made by the learner that match acceptable phrase alternatives entered in your moment or conversation simulation node. 

Intent AI works differently. 

With Intent AI, we don't limit Learners to using specific words or phrases. we let them speak in their own, natural tone and style. But we DO expect them to match the meaning and tone of the approaches and standards we've trained them on. To do this in the system:

Step 1: Provide acceptable statements that both meet your expectations AND have similar approaches. We've found it usually takes 3 intent samples to get good results. 1 is too narrow, and 2 may not provide enough diversity of language, which can cause good learner submissions not to hit as expected. 3 to 5 samples is a sweet spot. 
Step 2: Click the Search Icon in the Explanation section to bounce your intent statements off of the Bright LLM. The result will be a generative , 3-4 sentence summary of your Intent, based on your sample statements. Make sure you AGREE with the generative AI summary. If you do, you'll get great results from the Intent Match feature. If you don't, you need to add more sample intent statements with a little more diversity of style/language, but that still meet your expectations. 
Step 3: Convert your Explanation into Instructions. 
You can reference the node view where you'd execute these 3 steps below.

Tips for Designing Intent AI 

The steps for entering Intent AI are super easy. But there is definitely an art and a science to making this approach work. Here are our top tips for getting good results, fast.

Use Both Full and Partial Intent in Your Rules in a Conversation:
To design this feature we partnered with amazing industry experts in generative AI. While working, we realized that sometimes the LLM detected SOME fit between learner statements, but not a FULL match. 

For example, if the intent was to 1) Provide a warm greeting, 2) note the company name, 3) note a recorded line, and 4) ask for the customer's name. A full match according to your samples may be something like "Hello, thank you for calling City Bank. My name is Rob and we're speaking on a recorded line. With whom do I have the pleasure of speaking?"

This is clearly a better fit for the intent than a learner who says 'Thanks for calling City bank. We're on a recorded line. May I have your name?' The Learner may also leave out 1 of the 4 elements, which is 'close' to the intent, but not a true match.

To reflect this we offer both Full and Partial Intent Match in the dropdown (noted below). The best practice in most simulations is to enter BOTH match types with the SAME intent samples. In this way, you can allow learners to proceed in the conversation simulation if they don't match intent perfectly. This also allows you to provide lower star ratings + different coaching to the learner for partial intent matches. 

The best way to see how flexible the Partial Intent match really is will simply be to test!

Intent Dropdown

Stay Humble - the LLM is Pretty Darn Good At Defining Intent: 
There may be times when you DISAGREE with the LLM analysis of your intent samples. Before you add more samples or decide 'the AI is broken' take a deep breath and re-read your entries. We've been very pleasantly surprised with how good Bright Intent AI is at picking up on subtle intents in samples. 

For example, during design we noticed that if we used intent samples that wrote out "thank you" and used more formal terms like "ma'am" across all 3 statements, the LLM may deem a learner submission as 'Partial Intent' due to an informal tone (e.g. using 'Thanks' or words like 'cool'). 

Test Before Releasing to Learners: 
One of the core AI principles at Bright is to stress test and improve AI conversations and coaching with sample learners before releasing experiences in a live program. NEVER just build your rules, test in the node, publish, and start training learners. While it may be fast to click the buttons and build the experience, you'll get much better quality results if you use our other features to verify the rules are working as expected before using them in formal company training programs. 




Refining Intent Match Explanation Statements
Once you enter your sample statements, refining the AI-generated explanation statement is crucial.
The Intent Match AI algorithms uses an LLM to analyze sample statements that you enter. Learn more about the basics here. The statement analysis is called the "Explanation" - and they're pretty incredible! 

That said - they're not often perfect. Let's break down how the team here at Bright thinks about the process of creating great rules using the Intent Match algorithms:

Step 1: Gather real-life sample statements!
The more your sample statements sound like real employees, the more effective the rules will be! Pull from calls, transcripts or a listening tour. 

Can't make one of those options work? Consider asking a few colleagues how they would say it, or even ask an LLM like Anthropic's Claude or ChatGPT to brainstorm a few options with you. 3-5 sample statements is the sweet spot. Remember to vary your verbiage and keep required elements consistent- and yes all of your QA criteria should be met.

Example sample statement:

"Thank you for contacting Bright. My name is Rob. May I have your name please?"
Here, let's assume the requirements for our call greeting are to show appreciation, mention Bright, introduce yourself, and ask for the caller's name. Here are additional samples based on this criteria:

"Thanks for calling Bright. Sarah speaking. Who do I have the pleasure of speaking with?"
"We appreciate your call to Bright. This is Max. Can I have your first and last name please?"
"Thank you for calling Bright. I'm Aubrey. May I have your full name?"
"Thanks for contacting us. You have reached Matthew with Bright. Can I get your name please?"
Step 2: Generate + Review the LLM's Explanation
Once you have the samples, you can click the magnifying glass next to explanation, and the LLM will work to find the common ground in the samples provided. As a quick practice- look at the samples we provided above- what do you see as the common elements? 

Here is what the LLM thinks when we extract the explanation given these statements:

"The intent or meaning behind the collection of sample statements is to greet the customer and request their name in a polite and professional manner for identification purposes when initiating a customer service interaction."

The LLM in this example caught that each sample is greeting the customer and asking for their name. So you can see it is not perfect, but we're not done yet!

Step 3: Turn the Explanation into Instructions
Now that you've read the explanation, it's time to fine-tune it so that it reads as a set of basic, clear instructions. Think of it like providing Coach Guidance for the AI engine so that no matter what the learner response is, the AI has a clear set of instructions on how to rate their submission + what feedback to give. Be aware the AI grades based on the explanation- the intent samples only help extract an explanation- they are not factored into grading.

Here are a few of our favorite ways to provide instructions:

"The statement must include each of the following to match this intent: [insert list]"
"If the statement does not include X [or insert list], it does not match this intent."
"The response should be an X% match to the following statement: [insert scripting]" - strongly recommended for compliance, but not for other needs.
"Just including X is not acceptable for this intent"

So let's revisit our previous example and a few other common explanations. Pay attention to how easy each of the Edited options are for you to follow without seeing the Sample Statements!

 
Step 4: Test!
Use the Test a Submission feature below your AI rules to conduct preliminary testing. This testing should be two phases: 1) test using your sample intents to make sure you've written an Explanation where these samples DO earn credit! 2) test using common unacceptable answers + review the explanation as to why they do or do not earn credit. As you test, the submissions will display WHY they graded passing/failing- review this especially when it is inaccurate to see which part of your intent may need clarification.

And, as always, release the Moment or Conversation to a testing group before putting it in front of learners!




Conversation Simulation Basics
There are 8 elements of the Conversation Builder we think you should know about.
Core Convo Builder Elements: 
Learner Experience: 
The end-user experience combines a software simulation with a collapsable chat thread + star rating progress indicator. If there's no software simulation canvas the learner experience is just a clean conversation thread. Check out both views below.

Screen Shot 2023-09-20 at 7.33.05 PM

Screen Shot 2023-09-20 at 7.33.51 PM

Learner Practice History: 
Before a learner makes a simulation attempt, they'll see some kind of preview outlining the Stages + scenarios they'll face. This same page provides a quick link to see the contents and scores from their prior attempts.
Screen Shot 2023-09-20 at 7.36.14 PM

Learner Debrief Page: 
After a full simulation attempt learners will see a recap of their performance, including tips for their next attempt + the full conversation transcript. 

Screen Shot 2023-09-20 at 7.37.48 PM

 

Adding AI Conversations and Moment Groups: 

Simulation Buttons-1
Add AI Conversation simulations can be added by clicking the 'Add AI Conversation' button. (*They are not a Moment Type).  You'll have a window where you can type the name of your new AI Conversation.  You'll have the option to click the checkbox to 'Use LLM' and click SUBMIT.
Add AI Conversation

Once you've created an AI Conversation you can attach a Canvas if you want the conversation to be accompanied by a software simulation. You can also set one or multiple performance thresholds required for a learner to 'pass' a simulation. 

Screen Shot 2023-09-20 at 7.38.47 PM

You'll see your new AI Conversations and Moment Groups listed, where you can either edit, delete or rearrange the order of the items by dragging and dropping.
Simulation Buttons 2

Builder Tool: 
The main builder tool features a few core views for defining stages, creating 'nodes' inside the conversation, and routing conversations.

Builder

Inside each node there are multiple views where you can add responses, coaching, star ratings and the AI rules that assess learner performance. 

Screen Shot 2023-09-20 at 7.42.54 PM

Synthetic Voices: 
When you create a simulation you can choose from 3 customer/patient response options: Text-Only (which is a Chat), Audio (through which you can record or upload human voices) or Text-To-Speech (through which Bright provides amazing generative AI voices from multiple providers). 

Screen Shot 2023-09-20 at 7.45.49 PM

Screen Shot 2023-09-20 at 7.46.31 PM

Analyze Mode: 
Once you publish a simulation design, you can switch to Analyze Mode to see learner submissions for each node and analyze how they're performing. The main view shows the #/% flow of learners through your design. If you open a node in Analyze Mode you can then see the details of each submission. Remember that Analyze Mode is READ ONLY. This is only for analyzing historical submissions. 

Analyze 2

Analyze 3

Training View: 
Training View is similar to Analyze Mode, but is designed to test/update your AI model, coaching, star ratings, and other simulation elements. In this view you can change/edit rules, refresh the page, and see how the changes WOULD have been experienced by learners with those types of submissions. The goal is to iteratively test your changes until they are optimized for the largest number of learner scenarios.

Training View

Publishing:

The AI Conversation Builder allows you to make edits over time, but the key here is that for those changes to take effect we have to publish the new version of the convo builder. We do this using the carrot dropdown near the preview button. Then, select Publish.

Publish Button

So let's talk about what that means. In a nutshell, each time we publish, a new version is contained in the convo builder. To find the prior versions, we need to be in the Analyze mode then navigate to the 3 dots near the current version. Switching versions is meant to help us understand how learners responded in prior version history. This toggle gives us that power.

Version History

Now the catch is that we do not want to end up with 30 versions. So let's apply prior best practice of training a new AI rule through the coach inbox. We should test our initial convo builder with a core group of our admins/coaches or learners (still 10-15 is a great goal). Next, we should use the analyze mode described above to view their submissions and identify possible edits. Then once we make ALL edits from our testing cohort, we can then publish. 

This is especially important to keep in mind when the convo builder is live to learners. We have to remember to publish for changes to kick in, but the point here is to not overdo the number of times we hit publish it as it makes reporting more complex. The last bit on this is the Simulation Skills Report. You may notice an * on certain Learners- this is to identify if the scores showing on the table were from a prior version. Depending on the degree of changes we have made between published versions- we may need to take the score on this table with more than a grain of salt.




How Do I Design for the Convo Builder?
The Convo builder is an AI-powered tool to deliver conversation and chat simulations with simulated customers or patients. Designing simulations is both an art and a science.
This topic is WAY too big for a single knowledge article, but in this article we can definitely cover enough to make you dangerous.

How Conversation Simulations Appear in Assignment Paths: 
Before you design, it's good to think about the end product through the lens of the learner experience. The good news is that there's nothing too special about how conversation simulations show up in reports + learner assignment flows. They appear in the Simulation/Course page view just like moments, and also appear in Team Progress and Assignment/Outstanding reports just like moments as well.

If you've added the Simulation in which one or multiple conversation simulations are contained into an Assignment Path, you're good to go!

Note:  Conversation Builders must first be published to be visible on the Simulation/Course page. Until then, the Moment Group will be invisible to the learner.

First Things First: Starting an AI Conversation Build
Click here to watch
Quick guidde
1. Open a new Simulation and Click "Enter a name for this simulation." and Name your AI conversation simulation.

2. Fill in the same of the simulation

3. Click "Save" - Save your AI conversation simulation.

4. Click "Add Moment Group" - Create a new Moment Group.

5. Click "Enter Moment Group's Name... and Name the Moment Group as desired.

6. Fill in the name of the conversation

7. Check "AI Conversation Builder" box.

Check 'AI Conversation Builder' box.

8. Click "Submit"

9. Click into the Moment Group - Click the selected option

10. Add a description for the conversation. - Add a description for the conversation.

11. Fill "Type a description for the learner here. - Provide a description for the learner.

12. Click "Save" - Save the changes made.

13. Click "Edit" - This is how you will access the AI Conversation Builder!

Suggested Design Process: 
Designing conversation simulations is a new and different skill than designing eLearnings, and even a different skill than stand-alone Moment exercises. You should definitely review the Bright Academy video lessons on this topic as well, but as a recap on the design process suggested there...

1) Write a summary of the simulation you're going to build + the associated skills before you open the builder tool.
As tempting as it is to just start building: don't do it! You will save considerable build time if you pause before building to clarify key details essential to good design decision making. Your summary statement should answer things such as:

Who is the customer/patient the learner will speak with in the simulation? What's their problem? What's their personality/dialect/age?
What skills, behaviors and habits do you want to build in the learner?
What are the performance thresholds you'll use to measure those skills?
Is this a beginner, intermediate, or advanced level simulation?
What's the basic flow of what will happen at the beginning/middle/end of the call?
Are branching threads needed to achieve your learning goals?
Will a Canvas/System simulation be delivered in parallel?
How many Stages will you use for the simulation and why?
Will you use text-to-speech AI voices, human-recorded audio, or text only?
If you can't write answers to these questions in an email to a colleague, you're not ready to build a simulation yet!

2) Collect 10-20 calls with a similar scenario.
While it's possible to sit back and 'write' a simulation, we've found its faster and more lifelike to use transcripts of real calls. We recommend you collect 10-20 so that you can have objective examples of what associates and customers typically say in these settings. If you review these transcripts as you write your call outline, you'll instantly become MUCH smarter about ideas for side-paths, non-linear call flows, common mistakes, customer tone, and so much more. 

3) Build a baseline simulation with a single golden call.
Now you're finally ready to open the Builder Tool. Pick one of your calls with strong performance throughout and create Stages and baseline entries for a single, linear flow through the simulation. This will give you a shell of a conversation around which you can work from here on out. In the case of Beginner level simulations, this may also be pretty close to the level of detail required for your final product!

4) Create at least 3 responses per node. 
Next, revisit each node and think about the concepts of partial credit and common mistakes. In our experience, nearly all nodes would benefit from having at least 2-3 responses which provide partial credit (e.g. a version of a response that shows empathy or doesn't; a version of the response that follows 1 compliance requirement but not another). Give these responses different numbers of stars to reflect that one is more valuable to the company than another. Similarly, you can think about the common mistakes you saw in the transcripts you gathered at the beginning of the process. Enter response options that anticipate what will happen if the learner makes one of these mistakes.

5) Write thoughtful dead end and response coaching.
The Dead End response is a mandatory field in all nodes. If a learner meets no other response paths, they see this coaching as a hint to help them try again and move on in the conversation. You need to make sure the coaching and hints in the Dead End are meaningful in each and every node. If you put yourself in your learner's shoes, if the coaching is too generic, they won't know how to improve their response when they try again!

Note: If your Dead End coaching is empty or too generic, a learner could stuck in the simulation or have to exit the attempt altogether! 

Additionally, remember that your coaching will serve the dual purpose of also providing end-of-simulation recap notes for the learner. The table below is an excerpt from the post-simulation debrief page. As you can see, your Coaching Entries will be lined up as a sort of 'checklist' for things the learner should improve next time. Writing with the end in mind makes a better overall learner skill building experience. 

Screen Shot 2023-09-20 at 7.14.38 PM

6) Add a Canvas + Preview. 
Don't let the perfect be the enemy of the good. If there's an associated Canvas with the conversation, attach it, Save, then Preview the simulation. Share it with colleagues for early feedback. If you can't get through a simple version of your simulation, you certainly won't be able to get through a more complex one. 


6.5) Nap break. 
You should be able to do everything above in a single sitting. But you'll want fresh eyes and a clear head before you do the next steps. We recommend stepping back from your build and coming back the next day at this point in the process. 

7) Revisit your other calls to layer in sub-optimal routes, non-linear flows, or more rigor. 
Now it's time to go back to your call samples and look for ways to make the simulation more lifelike. The two main ways to do this are: 

- Create Sub-Optimal Paths: One of the most fun things to do in conversation simulations is to let learners 'live with their mistakes.' For example, if you were building a sales simulation, you may want to have a couple paths that end with a closed deal, and another where the customer hangs up on you or just says, "No." To do this you need to think through WHY a learner may get one path or another, and spend some time crafting these side paths. 

- Repeat Certain Nodes for Non-Linearity: In some scenarios, certain topics could come up in any order. We call this concept 'non-linearity.' For example, in a medical conversation simulation a provider may ask questions about a patient's symptoms in any order. This is doable in Bright, but you'll need to take some time to clone your core nodes and re-wire them so that a learner can ask any question at any time. This isn't really an AI thing so much as being super organized about where all your arrow lines are pointing. 

Word to the Wise: There is such a thing as making your simulation 'too smart.' Just because you can add side-paths or non-linearity doesn't mean you should. In most cases there's nothing wrong with a learner hitting a Dead End because they tried an unexpected approach. We've found that learners will embrace your guidance to ask them to follow a different approach.


8) Test + Improve.
Run through your conversation builder as a learner to get the full experience especially if you choose to tie a canvas to the conversation. Once you've done this, the best thing you can do is let 10 or so testers attempt your simulation 2-3 times each. Tell them to 'break it if they can.' The more diverse their statements, the better the data you'll have to use in Bright's Analyze Mode + Training View features, which let you test new response options, coaching, star ratings, and AI rules in real time. 




How Can I Write Amazing Simulation Coaching Feedback?
This is arguably one of the most important skills for a conversation designer to master. Here's why it matters and some tips on how to do it well.
First - what is Coaching Feedback and how does it play a role in delivering conversation simulations? 

Each Node Response option has a box marked 'Feedback' that is used after the learner makes their statement OR if they hit a Dead End. 

Coaching Box

Here's that same coaching entry as the learner sees it during the simulation... 

Jamie Sample

and afterward in the Debrief page, where it helps explain why the learner received 1 star instead of 4 for the skill of Account Verification. 

Post Report

The key to writing great feedback is to write with both use cases in mind. 

Coach Writing Tips: 

Be specific. 
Don't use generic feedback like 'Great job.' or 'You didn't follow our call flow.' This doesn't tell the learner what they specifically did well or specifically need to change to do better next time. Put yourself in the shoes of a new hire trying to learn 100 things at once. If they read your Feedback entry after a simulation and want to use it as a checklist of things to try in their next attempt, will what you wrote help? 

Note Context. 
Feedback is a great opportunity to build the learner's intuition about how to read the subtext of what's happening in a conversation. Great feedback points this out explicitly. For example, let's say during a conversation an associate asks for the customer's date of birth to validate an account. If the customer replies, 'Sure - It's September 9, 1985. Actually I just celebrated another one last week - I'm getting so old!' 

If your Feedback says 'You didn't build rapport with the customer.' many new hires will have NO IDEA WHAT YOU MEAN. 

The better way to write Feedback in this case would be 'Did you notice that the customer just mentioned her birthday was last week? This is a great opportunity to acknowledge this and build some rapport by saying 'Happy Birthday.' Give this a try next time around.' 

Repeat + Reference Prior Training. 
The principles of spaced repetition and the Forgetting Curve are alive and well. The reason your learner may have made a mistake while trying to apply your call flow or customer experience standards is likely because they forgot! We're big fans of briefly summarizing key takeaways from prior training using the exact same language you did in the prior training. 

For example, let's say your company requires that employees capture 2 of 3 unique data sets for a proper security authentication. An example of weak Feedback would be 'Don't forget to capture the required authentication points to meet out security standard.' 

An example of great Feedback that references prior training might be something like "During your "Compliance Essentials" lesson we covered the 3 options for proper authentication: DOB, mailing address, or amount of last transaction. You only asked for 1 of these, so didn't meet our standard."

See what we did there? We didn't just say 'get all the authentication data' - we repeated the standard, then noted the opportunity to improve. Write this kind of coaching across a 2-3 week training program and your associates will be crushing it in no time!




How Publishing Impacts Reporting in Conversation Simulations
Conversation simulations allow you to work on future drafts of the experience that are different from the live, published version. But you should publish new versions sparingly.
At first glance, being able to publish multiple versions of a conversation simulation may sound like a great thing. But there are a few downsides to publishing versions that Admins should know about to help them optimize the reporting and AI training features in the platform. 

To provide background, it's worth taking a quick step back to point out that as you evolve your conversation simulations, new nodes will be added and response and star rating entries may change. 

This means that Learner A - who takes your simulation this month - may go through a node that Learner B - who takes your simulation next month - may not see at all. 

This dynamic means that we literally - mathematically - can't combine simulation results in many cases. 

Why should you care? 

Because this also means that if you publish too often, you may lose some reporting capabilities. 

Let's Be Clear: Here's What Will NOT Be Impacted

Before you get concerned, we want to point out that for the most part this isn't a big deal. 

Learners will be able to see their data and experiences across multiple versions; they'll likely never notice that you re-published a version.
Coaches will be able to run reports with no issue regardless of how many versions there are. If Learner A went through a different version than Learner B, we can show their respective scores without hurting the reports. 

But Here's What May Be Impacted

The Analyze Mode + Training View Reports in the Admin Panel will only be able to show numbers and % from one version at a time. 

For example, in the report below this snapshot is from Version 4 of a simulation, which has 17 submissions. 

Report Sample

Here's the same report for Version 5... Only 1 submission so far, and the nodes have changed. 

Report 2

The same dynamic is at play in the Training View, where you can use prior submissions to test your AI rule drafts and make them better. The filter below actually shows the count of submissions to each of the versions + their publish date. 

This view is a great illustration of why you probably want to limit your published versions. The more submissions you have in a single version, the more data you can see during analysis + AI rule fine-tuning. 

Publish Versions

Tips For Publishing

Use the Preview:
You can see how your build is working in the Preview without negatively impacting your numbers. You can also send the Preview link to other Admins so they can do the same. 

Get More Testers Before Release:
Prior to releasing your conversation to actual learners, you'll still want a decent amount of data to fine-tune your AI rules. Identify 5-10 testers and have them 'try to break' your simulation. If they each submit a mix of expected and unexpected replies, you'll have lots of data in your Analyze Mode + Training Views to tweak your rules and improve before you release to learners. 

Limit Updates Once Learners Start Using a Simulation:
Once simulations are live, unless there are egregious errors or bugs, we suggest you let at least 50 participants go through it before making changes. This will give you a good data size for testing/determining improvements so that the next 50 have something even better. Our Simulation Skills Report in the Reporting Tab will show an * for any scores completed against a prior published version of the conversation.




How to Use Conversation Simulation Thresholds
Bright has 7 unique performance thresholds that automatically re-assign conversation simulations to Learners until they pass. But that doesn't mean you should always use all 7!
Here are the 7 metrics you can optionally set to define 'passing' a conversation simulation. 

Thresholds

Conversation Score: The % of total possible skills stars earned by the learner during the simulation. (e.g. 15 out of 20 stars = 75%)
Duration: The total handle time of the conversation. (e.g. 4:32)
Avg. Response Time: The average amount of time a learner takes before starting to respond to a customer's last statement. This is an approximation of 'dead air' time, and is typically better if it is lower. Higher Response Times indicate the learner is really having to stop and think about what to do. (e.g. :32)
Optimal Responses: The number of statements made by the learner that routed through response tabs marked with the 'Optimal Response' checkbox. This is usually an indicator that they are following their training and/or your quality standards. (e.g 8 Optimal Responses)
Dead End Responses: The number of statements made by the learner that met none of the conditions in your response tabs, and that therefore required coaching/hints. Learners must repeat their statements until they match one of the response flows, so it is possible for a learner to hit a Dead End multiple times before proceeding. (e.g. 3 Dead End Responses)
System Errors (for conversations with attached canvases only): The number of times a learner tried to proceed on a Canvas screen with incorrect form fields, dropdowns, missed toggles, or other validation errors. Since a screen may have multiple validations on the same screen, the same screen may trigger more than one System Error. (e.g. 10 system errors across a 20 screen canvas)
Lifelines (for conversations with attached canvases only): The number of times a Learner has to use the blue 'i' button to request a hint for system management steps. This operates the same way that our System Moments Lifelines do, only in this case can be set as a threshold. 
Learners see these same metrics when they complete a simulation. As shown below, only metrics with thresholds will show the 'Target' text next to the number. Thresholds set to 'Any' will remain blank on the learner page. 

Debrief

Here's our guidance on they why and how of using different types thresholds: 

You should think of each simulation in terms of difficulty level. We highly recommend staging difficulty levels for training programs from Beginner - > Intermediate -> Advanced. 
Based on this, we think having ZERO thresholds is a great way to start for Beginner conversation simulations. The process of 'putting it all together' takes learners a while, and zero thresholds provides safe space for practice. 
For Intermediate difficulty, we recommend starting with Conversation Score, Duration + System Error metrics. These 3 are the most telling for the majority of simulations. 
For Advanced Simulations, you can start to use other metrics such as Optimal Responses + Dead Ends. These metrics force a level of accuracy on the learner. For example, if a learner can have zero Dead Ends, that means they have to get through the simulation with NO COACHING OR HINTS. This is reasonable for someone with lots of practice and training, but too much to ask of a newbie. 




Convo Builder Node Favoring
How do I change the favor on a node to give fewer or more stars in different use cases?
When a node has multiple responses, ensuring that your learners receive personalized and accurate feedback becomes crucial. Let's imagine that a learner's response fits two different responses that you have built. The default setting for scoring inside the conversation builder is to be generous and award the most stars possible based on their submission. However, you can override that default setting by configuring the score favoring. This allows you to customize the number of stars given based on different use cases.

If your node design is best-suited for a learner to be awarded a higher star rating if they meet the requirements of multiple responses on the node, no changes will be necessary. But what about when the verbiage a learner uses should earn the lesser rating?

Favoring More or Fewer Stars: Tailoring Learner Feedback
Giving Fewer Stars for Specific Criteria
Let's imagine that you are working on a conversation and set up two very similar responses, but one includes a common error that you know learners are making and want to train around.

The learner needs to put someone on hold to do some research because the last person to work the case did not add any notes to the CRM. The optimal response is "OK, John. I am going to need to place you on a brief hold. Is that OK?" The common error you are seeing is that agents are telling the customer that there are no notes on their account and you want to make sure this always loses them stars on their responses. 

So, what happens if a learner says "Hey John. I'm so sorry but there are no notes on this record so I'm going to have to put you on a quick hold for a second. Is that ok?"

Enter: favoring fewer stars. This means that the AI Engine will look for the lowest appropriate score instead of the highest and weigh the common error before the correct behavior of asking to put the customer on hold.

Node Favoring allows you to customize feedback based on specific criteria, promoting a more accurate assessment of learner performance. To implement this, follow these steps:

Click on the three dots within the specific node in the Convo Builder.
Node Favor More Fewer Stars
Explore the favor options to choose the preference you want the node to possess. Here we would Favor Fewer Stars
Giving More Stars for Positive Interactions
Consider a scenario where your node has two responses, both offering warm greetings. The distinction between a 3-star and 4-star rating could be as simple as asking for the caller's first and last name. By default our node will favor more stars, but if you had previously adjusted a node to favor fewer stars or cloned a node with this favoring, you can change it back by selecting "Favor More Stars".

Handling Multiple Responses
If you have multiple responses with the same star rating, the default behavior is to use the first response in order. It's advisable to design your conversation flows considering this feature.

Node Favoring in the Convo Builder is a subtle yet powerful tool that allows you to enhance the learner experience by tailoring feedback to specific use cases. Whether you want to give more stars for positive interactions or fewer stars for certain criteria, Node Favoring gives you the flexibility to create a more personalized and effective learning journey.

Node Favoring: Example Case Study
Let's use an example here of a common practice that prompted this feature. In our call center, we will say the learner is an agent fielding calls from a customer of our product. The agent opens a CRM for a call from the customer looking for an update on a case that was reported and escalated yesterday. However, when the agent gets into the CRM, there are no updates on the case.

Now we think about the Convo Builder- we reach the node of this conversation where our agent should be providing information to the customer. Let's say we added 3 responses on this node with the following star ratings:

1 Star- Phrase Match with terms "No updates", "Not updates", "No notes"

2 Star- Phrase Match with terms "Still working", "In queue", "Update soon"

2 Star- Phrase Match with terms "Normal turnaround", "24 48 business hours", "1 2 days"

Based on this, our default would show the following ratings for the responses from learners:

Henry: "Unfortunately I do not see any updates on the case, but our normal turnaround is 24-48 business hours"

Lexi: "From what I am seeing on your case, it is still in our queue, but you should be hearing from one of our support specialists within 1-2 days from when your case was escalated."

In this case study, Henry should be receiving 1 star due to the negative word choice they used. By default however, both of these agents are receiving 2 stars since node favoring will naturally award the highest possible star rating. Henry met rule 1 and rule 3- since rule 3 is 2 stars, the favor went with this option.

Here is where we would edit the node favor to "Favor Fewer Stars". This setting would trigger Henry to get the 1 star response as we intend. 

Lexi's response demonstrates how we should consider our building process. Lexi hit rules 2 and 3, but since they both are 2 stars, the feedback Lexi will see is for our rule 2 ("Normal turnaround." This is not a setting that will be editable, but a callout to keep in mind when we are designing nodes.




Conversation Builder FAQs
This article addresses some of our Admins most common FAQs about the Conversation Builder.

Designing a Conversation
How should I incorporate simulations into my existing Bright program? Should they replace my Moments? No way - this is an incremental feature to make your programs even more powerful, not a replacement of existing programs. We suggest that you use more traditional moments or simple, linear conversation simulations early in a training program, and build your way up to more complex/branching simulations later in the program. We're huge fans of scalable AI-powered experiences, but also see significant evidence that hybrid learning programs that utilize spaced repetition over time get better results. Speak with a Bright Account Manager if you want to brainstorm further through this topic.

Why would I want to create a "zero star" node? Conversations have a lot of "minor moments" that simply move the dialogue forward. It may be better to use zero stars if you want to keep the scoring and coaching simple so that learners focus on the "big picture" of the learning journey.

How can I make sure my simulation "ends" the way I want it to? A conversation simulation ends when BOTH the conversation and software simulation flows are completed UNLESS you click the "End Simulation" checkbox in the associated Node. If you do this, it will end the simulation regardless of where the learner is in the conversation or system.
Can I use conversation simulations in other languages? If you're using common Romance languages like Spanish and French, yes! Just enter the rules, dialogue, and canvas components in the same language and it will work. But if you need other languages, you'll need to let your Bright account manager know first.

Building a Conversation
What will happen if I add a convo simulation to an existing simulation/course page? A new conversation simulation won't show on the learner view or reports until it is Published for the first time. But, we strongly recommend you build a conversation simulation in a NEW simulation with no other content.

Hey - how come my synthetic voice sounds a little different each time I save? One of the API voice providers Bright uses is Eleven Labs. This is based on true generative AI capabilities, so each time you save a new customer/patient statement, it changes the inflection slightly, making it sound more lifelike. You can re-save OR adjust punctuation and copy to get different results. For more info here check out the Bright lesson on synthetic voices.

Can multiple team members work on the same simulation at once? Not at this time - we plan to add this capability in the future, but for now, we recommend only 1 admin works on a conversation simulation or AI fine tuning at once.

Can I clone a conversation simulation? Yes! From the simulation landing page, the conversation builder moment group tile has a copy button. Once you select this, you will be able to choose whether the clone goes to the same simulation, an existing simulation, or even a brand new simulation!

Why can't I delete my conversation stage? We only allow users to delete EMPTY, FINAL stages. If you delete all nodes from your last stage you'll see a trash icon appear. You may need to drag/drop your nodes into interim stages to make this work, but don't worry - the connections/flow will hold when you drag and drop.

What happens if I save a conversation simulation and a branch is not connected or coaching is not included, etc? If you try to save a simulation with missing connectors or empty AI/Coaching fields, you'll notice a friendly popup pointing this out. It's fine to continue with publishing if you're just testing, but we do suggest that you aim to address these points prior to publishing.

Will I lose my audio recordings if I change an audio conversation to text? No - we'll just hide the audio files if you do that. If you switch it back to audio, they'll magically re-appear!

Is there a limit to the number of responses or branches each node can have? No - the only limit is your sanity and logic skills. But we recommend limiting your use of response options and branches during early builds. There is such thing as a simulation that is "too smart." Remember that your goal is to accelerate learner up-skilling, not create a "perfect" simulation.
Can I revert to a prior version of a published simulation? Not at this time.

AI Rules:
How does the phrase match actually work? There can be up to 10 "tokens" between the words in your term/rule. So have a great day could be "have a really wonderful awesome day" but not "have a really wonderful awesome terrific day." As context, tokens are sometimes but usually not the same thing as full words.

Analytics + Reporting:
Wait - my Analyze View and Training View are a little different. What's up? To make multi-branching simulations work from a technical perspective, we had to create "versions" of published simulations. This means that since nodes/rules/coaching/routes can change between versions, comparing "across versions" doesn't always work or even make logical sense. So the Analyze view only shows historical learner activity/results, while the Training View allows you to edit rules and predict future learner activity/results. Watch the Bright Academy lesson on these functions for more info! 
Can coaches see the conversation thread shown to a learner after their simulation attempt? Yes! If you open the Reporting Tab and go to the Simulation Skills Report, you will be able to select the team and the conversation you would like to view. Then, click the Learner's name in the table. You will then see a page nearly identical to what the learner sees when they go to the conversation. Admins can also view learner submissions in the Analyze and View node of the conversation builder while tweaking your nodes/AI rules.

Learner Experience:
Can I force a learner to complete the canvas? Yes, just skip using the "End Simulation" checkbox in your conversation build and the simulation will not end until they have completed both the last node and the canvas.

Can I force a learner to only use audio? Not at this time.
If a learner stops a simulation mid-way, will we save their progress? Yes and no. We will save an attempt and all progress made up to that point, but if they start again they will have to begin the simulation from scratch. This protects your company from having learners "game" the simulation to get perfect scores.

Can a learner get "stuck" in a simulation? Yes - if your response AI rules are too narrow or the context of the conversation is unclear, it's possible a learner would keep getting the Dead End prompt. We HIGHLY recommend you do a final editing review of your Dead End prompts before publishing to ensure your coaching effectively helps learners finish the simulation.

How do you "pass" a conversation simulation? This is one of the most existential and complex learning questions out there today as it could be based on conversation or software skills or both. Do you have to pass all stages? Get a high overall score? Make zero mistakes? Provide all optimal responses? There's no single right answer. That's why we've enabled "pass" thresholds across multiple metrics/criteria. A simulation with no thresholds will be "passed" after 1 attempt. A simulation with one or multiple thresholds will be "passed" as long as all criteria are met (just like in other moments today). 

Simulations will be resurfaced for spaced repetition in MyPath (just like traditional Moments) until all skills criteria are met. See this help article on each of the threshold types available and our guidance on when to use which types.
What happens if a learner passes a Convo Builder and then does not pass the next time - what shows in the reporting? We've paid special attention to this scenario to avoid confusion for the learner! Once a learner passes a conversation simulation we will permanently note them as 'passed' in all reports. For example, if they continued to practice a simulation but 'failed' a later attempt, this would not negatively impact their MyPath page or your reports. In this way, learners can keep practicing as much as they want once they've passed.





Creating a Teams Structure
Learn how to use Bright's 4 tier team structure.
Teams in Bright are a way to segment your learners into smaller groups or catagories. It drives all of the reporting in Bright, as well as your decisions on what kind of content you build in the platform. Tagging content to a certain team it is overall the best way to think about organization in the platform. This is a key step as you onboard or begin to expand deeper into your organization. 

The first thing to know is that Bright Teams are constructed using a nested, hierarchical, tiered structure. There are four tiers that you can drill down to for reporting, coaching, and assignments. Tier 1 is your parent Tier and all of the tiers underneath that nest into each other.



There are a million ways to slice and dice your data, but we recommend using a similar structure to the way that your organization is structured. Do you think about your talent by division, region, talent band, or tenure? All of the above? Well, you're not alone! Working with your account manager will help you think through how to structure your teams, how you assign coaching tasks and how you view your reports.

Below are a couple of examples to get your juices flowing and help you think through different ways that you might want to segment your learners. Ask your account manager for this template if you'd like to use it to brainstorm as well.





Once your teaming structure is complete, it's time to assign Learners, Coaches and Admins to their teams. We recommend making the team structures before you bulk upload users. This way you can grab the Tier 4 Team Unique IDs to easily assign the users during the bulk upload process.

Tier 4 Team Unique ID

We'll also show you how to easily drill down in the Reporting suite to review team data. See tutorials below.

Filtering the Moments Report to Multiple Tier 4 Teams (Same process for the Coach and Assignment Reports)


Drilling Down the Team Progress Report to a Tier 4 Team


 

Filtering the Outstanding Report to a Tier 4 Team (Same process for the Simulation Skills Report)



Bright's Image Library + Badge Toolkit Link
Need a bunch of perfectly sized images for the platform? You've come to the right place.
Bright has curated a library of images that are perfectly sized for our platform. Ask your account manager to link the library to your shared folder or access it via the links below if you are interested.

Bright Image Library: includes perfectly sized Hero, Thumbnail, and Resource images
Bright Badge Toolkit: includes perfectly sized, customizable badges
Want to use your own images or have your marketing team make some new ones? Below we outline the different types of images that you will need in the platform and the dimensions for each. This will help you create a bank of images that are specific to your organization to use in the platform.

Image Library in the Admin Console
The image library in the Admin console is where you will house all of the images that you add to the Canvas layers including Toggle Images or other images you add to the Image layer. You can either drag these images directly into the layer when you are building a Canvas or you can navigate to the Image Library menu in the Admin console and upload it there. The image library in the platform is in alphabetical order. There are many enhancements coming to this part of the platform within the next year so stay tuned for updates here!




Managing User Access + Permissions

How to create individual users, bulk import users, and update permissions levels for users.
Bright enables customers to tailor the access and permissions of all users using a simple table format where are all attributes of a user can be edited and refreshed in real time. Follow the steps and instructions below to make changes for your organization's user access and permissions.
Navigating to the User Access Form
From the Admin Panel, select 'Users' in the left navigation bar.

Screen Shot 2022-10-05 at 10.11.28 AM
Filter + Bulk Editing for Existing Users
Once you've navigated to the User Table, you will see a list of current, Active users in alphabetical order. From this main table you can make bulk edits and filter the table to see certain group attributes. 

To filter: 
Navigate to the filter symbol in the upper righthand corner  Screenshot 2023-02-23 at 2.55.58 PM
We automatically filter the table on 'Active' users since these are the people most Admins are interested in and this is why there is a blue circle with '1' highlighted on the filter icon
When you click this icon, you are able to filter using a variety of different attributes including all Tiers of Teams, Assignment Path, Permission Level, Role and User Status
Screenshot 2023-02-23 at 2.59.51 PM
Once you select the filters that you want to apply, click 'Apply Filters'

To bulk edit:

Now that you've filtered on a group of individuals with similar attributes, you may want to make a bulk edit to their Team or Assignment path
Do this by clicking on the checkbox Screenshot 2023-02-23 at 3.05.55 PMat the top of this list - this will select all people in this group 
Navigate to the bottom of the list and select Edit Team, Edit Assignment or Mark as Inactive
Screenshot 2023-02-23 at 3.05.28 PM

Adding New Users
If you want to add a new user you can add one at a time or use our bulk import tool to add many users at once.

To add one user at a time:

Select the '+' sign in the upper lefthand corner of the User Table
Once you've navigated here, you will need to fill out the necessary details. Below is an overview of each user access field

All fields with an asterisk must be completed  
Once these fields have been completed, click 'Create User' 
Name Fields: Depending on the data provided by your HR/IT teams, there will be a series of name fields, such as Full Name, First Name, Last Name, or Preferred Name. These fields determine how the learner's name will be displayed in reports, as well as in their profile in the upper right hand of the platform. Preferred Name typically drives the 'Welcome' message that the learner sees on the Bright landing page.
Unique ID: This field contains whatever data point your organization uses as a unique identifier of employees. This may be an email, an employee number, or other similar content. For most customers, this field drives the Platform authentication process. Depending on your organization's authentication approach, without a unique ID, the learner will either be unable to access the platform, or will see no content when they arrive.

Email: this is the email address that is assigned to this user. 
Temp Password:  this field only appears if Bright is managing your active directory. Most clients use their own SSO authentication which means this field will not be applicable and therefore invisible. If the field is present, enter a temp password that you will share with new users. This must contain at least 1 uppercase letter, 1 lowercase letter, 1 number and 1 special character.

Role: This field typically corresponds to the learner's official job title. The role tag is often helpful for reporting purposes to show progress/performance across associates in a common role.

Permission Tags: Select Learner, Coach or Admin access. The Coach/Admin permission levels determine whether a user can act as a coach (e.g. rate Moments; host a Certification session) or an admin (e.g. create Moments/Sims; view reporting). If you Permission someone as a Coach, in addition to selecting which team they are on, you have the opportunity to select which Teams they will coach. The Coaching selection box appears when you permission someone as a Coach and you select the team that they coach in the same way that you select a Teams tag below. For more information on how to set these permissions, check out this help article.

Team Tag: Here you will select which team your user will be assigned to. The team selection modal will pop-up and you'll have to select each team tier your user will be associate with. This is extremely important and drives which reports that they will show up under and which coaches will be assigned to them. It's important to have your Team structure already in place before you add any users. For information on how to set up your Team structure see this help article. 
Assignment Tag: This field corresponds to which Assignment Path and the content that the user will receive when they login to the platform. It's important that you have at least one Assignment Path created before adding any users.
To bulk import users:

Click on the paperclip icon in the upper righthand corner of the User Table.
A prompt will appear and you can download the latest user upload template by clicking on the blue highlighted text in the prompt.


Ensure your file is in .csv or .xls format
There may not be any duplicates or errors to complete the upload. Errors occur when there are random spaces or unwanted characters in your spreadsheet so be careful if you are copying/pasting from a different file
TIP: when copying/pasting from somewhere else, select Paste Values only (PC Hotkey is Control+Shift+V and on a Mac is Control+Command+V then click Values from the list)
If there are errors or duplicates, we will alert you and you'll have to update your spreadsheet and try to upload again
When you are ready to upload, you simply drag the file into the upload icon in the above prompt
If your spreadsheet does not have any errors, it will ask you to confirm your submission

IMPORTANT: When filling in the spreadsheet, you must input the Team and Assignment path unique ID (not the name of the Team or Assignment). For more information on unique IDs in Bright see this help article.




How to Set Coach and Admin Permissions
Learn how to set coach and admin permissions in the admin console.
Once you have set user permissions to a coach or an admin in the platform, you will need to assign their coaching or admin permissions respectively. If you need help assigning permission levels, check out our article on managing user access permissions.

Coach Permissions
After you set a user's permission to Coach, you will see another selection appear for the coaching permissions. If you click this button, you will be able to assign the teams this user will coach.

Coach Permissions

You will click two saves in this process  one for the permissions and one for the user you are updating. Check out this tutorial for a demo.


 

A few things to note on this:

A user can coach one team or multiple teams depending on your preferences.
A coach will only see coach inbox items and reporting for the teams they are assigned to coach.
If the coach is assigned to coach the team they are on, they will not be able to coach themselves (coach inbox items)
Depending on the size of your organization, you may want to have coaches overlap for redundancy. If the primary coach is out of office, someone will need to keep up with the coaching.
Coach permissions can be updated at any point using this same process
Admin Permissions
Admin permissions are similar in that the button appears once the user permission is set to admin. Clicking the Admin Permissions button will open the same team selection window. However, admins are assigned to a Tier 1 team. This means an admin will be able to build and manage the content with team tags for their Tier 1 assignment. Just like above, you will save twice.
Admin Permissions

Super Admin
Super admins are the Bright Gurus of your organization. They will have access to all reporting, all elements of the admin console, and will be the people responsible for uploading users to the platform. To make someone a super admin, check the box that appears next to the admin permissions.


 

A few things to note on admin permissions:

By default, an admin will be able to see all content regardless of team tag until you assign them to a specific team.
All admins are able to access all reporting regardless of team.
Making someone a Super Admin will allow them to see and manage all content, regardless of their admin permissions selected.
Admins will see the following menu:
Admin Reduced Console Options
Super admins will see the additional options for items such as badges, assignments, users, and the welcome placemat.




Reporting Suite Overview
Let's review the basics of the reporting suite available to Admins and Coaches on Bright.
Bright's Native Reporting Suite includes numerous off-the-shelf reports that show both basic activity reports, as well as - and more importantly - the skill level and skill growth of each individual learner. The images and descriptions introduce the core reports you can use to drive your program results. Before you get started with Reporting, here are a few quick checklist items: 
Are your user reporting permissions set appropriately? In order to access the reporting suite in Bright, users must be set to coach or admin level permissions. Admins can see all of the reports outlined in this article, but coaches can see most of the reports. If there is a leader in your organization that needs access to view the reporting, reach out to your CX manager to get them setup.
Are your teams set appropriately? Most Bright reports are driven by team definitions. In order to pivot/group reports by team, make sure all users are assigned to the right team/group. 
Are your assignments set appropriately? Completion rates and report sequences are driven by assignment paths. Make sure simulations and moments are assigned correctly to ensure completion reports are taking all possible content into consideration. 
Are your Moments tagged? Some Bright reports use Primary and Secondary skills tags to enhance the reporting view. If you're ever looking at a report and looking for more/different detail on skills, make sure your Tags are set correctly!
And without further ado - here's your reporting suite intro!
 
**Report requires admin permissions to view

Reports Available to Coaches + Admins
Team Progress
This exportable, detailed report shows the completion percentages for each team in the platform. As you drill down into the tier 4 team, you will see each moment that is part of ANY of the assignments for this group of learners. For each moment, the users will have a designation such as 'Pass' that will be explained below.

Primary Use: Tracking completion and learner progress data at the team or individual level. Export this report into an excel file to easily manipulate the data into the format you need. If you are new to excel, do not fret! We have another article here, with some of the more common functions within excel to make your data clear.

More Information: We have good news for you: No more creating customer completion reports. Our permissions level admins and coaches can view team performance and completion all the way down to the lesson level. It's a powerful view. And honestly, it's fun!
 

Progress Bars (visible on tiers 1-3): If you click on the 'Team Progress' report tab, you can see an overview of completions and activity by Team. There are two basic completion numbers to consider. "% Completed' reflects moments that are truly complete - not just submitted for coach review. To be considered complete a rated moment must be attempted and then rated with a passing score by the coach. If the coach sends that moment back for practice/iteration, that moment will stay noted as 'In Progress.'   
Team Completions (visible on tier 4 level): If you click on a team you can then see more detail on any associated team members, with moment-by-moment real time information about completion status, and the number of attempts they've made on a Moment. The following designations can be found in the TPR:
Not Started - The learner has not submitted any attempts against the moment.
Pass - The learner has completed at least one passing submission against the moment.
Resubmit - The learner has submitted a response to the moment, but they did not pass and will need to resubmit a new response.
In Progress - The learner submitted a response to the moment, but the response is in the coach inbox pending grading from a human coach. This can occur on live AI moments if the learner exceeded the maximum submissions.
Blank - The moment is included in the table because at least one user on the team has it in their assignment path, but the user showing as blank does not have it within their assignment path.
Scroll right/left to get a high level view of team progress, and to note learners who may be behind (i.e., the gray 'Not Started' entries). If a learner passes a moment on the 1st attempt, we note that completion with a star.
Individual Profile Report
The individual profile report gives key insights into the skills and performance of an individual learner.

Primary Use: This report is perfect when preparing for one-on-one coaching opportunities to guide the conversation in both strengths and weaknesses.

More Information: Through the extensive use of simulation, coaching, tagging, and ratings, we're able to answer these questions in a way few others can. Because Bright measures skill and competency - not just knowledge - we can pinpoint learner strengths and development areas. As a result, we help our customers redirect limited resources to the people, teams, and topics that need them most. We can answer the question 'Are my people ready?' - and if they are not we can quickly point to why. This report is your first step on that journey.

While this isn't the 'first' report you'll see, we consider this report the most important in our reporting suite, so we'll start here. To find the individual skills profile report click the 'Team Progress Report' tab. Once you have drilled down to the tier 4 team the individual is on- this may be the default view if you only coach one tier 4 team, simply click on their name. 

 
Primary Skills Tags: The skills profile report is driven by whatever Primary and Secondary Tags are used on rated Moment types for each user. Across the top of the report you'll see clickable categories of skills. If you click on a skill category, the moment list underneath will change, showing the Moments tagged under that skill tag, and any associated performance data. The star score for the Primary Skills Category is a weighted average rollup of skills in that section. 
Moment Name: Once you click on a skills category, you'll see all rated moments grouped by simulation. If you're looking for performance on a specific moment, you can click the small arrow next to the Moments title to sort, or visit the Moment Report (more on this below)
Secondary Skills Tags: We note the secondary skills tags next to each moment as a reminder of what specific skills were rated. If a moment has multiple tags, the tags will each show in the row, however only the rating for the skill tag associated with the current Primary Skills Category will show. 
Attempt Scores: Each time a learner iterates on a practice moment, their star rating or system duration will be noted in a small graph. For rated moments, ideally you'll see a path of progressive improvement (i.e., the graph goes up) and for system moments you'll see a path of increased proficiency (i.e., the system duration graph goes down). If you hover over a data point you can see more details about that attempt.
Coach > Report
There is one report that can be found in the coach tab at the top of your homepage view. This report under the coach tab gives a quick summary of each interaction the learner has completed with the submission itself and a date/time stamp for accountability.

Primary Use: Quickly identify the certifications, moments, and simulations a learner has submitted. This can be used for accountability of a learner as well as following-up on error reports from a learner who had to resubmit a moment multiple times. This could also be used to view the AI feedback provided to a learner for a specific moment.

More Information: Once you navigate to the coach tab in Bright, select the learner's name from the reports dropdown.
Coach Tab Reports
Once you select a learner, you will see tabs for each type of experience in the platform. The most common option to select will be the moments to view each moment the learner has completed with the date/time stamp. This can be used to view submissions and ensure the coach provided appropriate feedback- regardless of a human or the AI coach. You can also use this view to ensure accountability if there is any doubt to whether a learner was on task- unfortunately none of us are impervious to this concern.

Coach Tab Reports B

Moments Report
This detailed report pulls out completion and performance data on one moment at a time. You can review the following:

What % of learners who have not started, are in progress, passed in more than one attempt or passed in one attempt
On Multiple Choice moments, you can see % breakdowns of answers selected by learners for each question
Number of individual submissions for each learner
Primary Use: Use this report to look for patterns and trends in performance, one moment at a time. 

More Information: Since the delivery of practice and coaching is new for most organizations, we blend content and individual reporting views to show how each piece of a simulation performs. For example, if you're building CRM proficiency, you can drill into system simulations and see time proficiency, missed clicks, etc. If you're testing soft skills in management or upselling, you can analyze how many attempts different teams/users are making.
To use this report, begin by selecting the moment name from the alphabetical dropdown list. Then filter by team to view the summary for the moment.
 

 
Moment Details: After you select a Moment from the dropdown to view the report, you can see a recap of the Moment and even click the hyperlink to re-visit the Moment to remind yourself what the exercise and prompts were. 
Attempts Summary: Most moment reports feature a pie chart that notes progress, including either completion status, or performance based on # attempts. Multiple Choice Moments also feature a breakdown of % selections of question answers to support analysis of question difficulty and content effectiveness. System Moments also feature a summary of average attempts and lifelines used for system simulations. 
Team Filter: You can filter moment reports by team to compare and contrast performance across cohorts. This filter can also help serve as a quick 'completion' report in some cases where you'd like to see Team performance on a single lesson. 
Individual Performance Snapshot: At the bottom of each moment report you can see individual learner performance, including status, number of attempts, and - for system moments - duration progress. If you see a graph, hover on data points to see more data about the attempt! 
Outstanding Report
The outstanding report gives a quick breakdown of each learner and the moments they have outstanding (ie. in progress, resubmit, or not started). For a breakdown of what each designation means, review the designation information here.

Primary Use: Use this report to help a group of learners cross the finish line by providing clarity on which moments are not yet complete and providing an estimate of how long it will take to complete these remaining items.

More Information: In the outstanding report, you will see each learner on the team and breakdowns of times and number of outstanding items to complete in their assignment path.

Oustanding Report Example A

Click on a learner's name to view the itemized breakdown of each moment outstanding for the learner. Keep in mind- in progress indicates a submission is awaiting coach feedback in the coach inbox.

Outstanding Report Example B

Assignment Report
Use the assignment report to track to completion of an assignment path for a specific team. This report is generalized and provides a high-level overview of the completion for a specific training assignment.

Primary Use: Quickly report the progress of learners against one assignment path to stakeholders. This report shows each simulation and each moment in the assignment path along with the designation counts for learner progress.

More Information: The assignment report shows an entire assignment path broken into the simulations that make up the assignment.

Assignment Report Example

Each simulation is broken into each comprising moment with its skill tags and the totals for how many learners on the team are assigned each moment. Of the total population, the report will also display the total number of learners who have each designation (ie. in progress, resubmit, or not started). For a breakdown of what each designation means, review the designation information here.

Simulation Skills Report
At the end of each learner's conversation simulation attempt they receive a real-time summary of their performance metrics + targeted coaching around any skills that need more work. This report combines the same metrics into a consolidated view for coaches.

Primary Use: Identify the strengths and opportunities for growth and reinforcement from the full conversation experiences.

More Information: To best understand this report, let's quick remember how the learner experience will conclude on each conversation builder attempt. As soon as the conversation (and possible the system simulation depending on the configuration) is complete, the learner sees this:

Post Simulation Learner Summary

Coaches and Admins can get a rollup view of this for their teams by using the Bright Simulation Skills Report. To access this, go to the 'Reporting' menu tab and select 'Simulation Skills Report' from the report list. 

After you select the team + simulation, you'll see a report that looks like this:

Simulation Skills Report

Key report features include: 

Completion Rate: this calculates the % of learners who have been assigned this conversation that have PASSED the experience. Remember that this report shows all team members, including those who have not been assigned the conversation. We only include those who have been assigned the conversation in the calculation.
Total Practice Time: This aggregates the total number of hours the team has spent practicing this conversation. This number includes BOTH partial and full attempts of the conversation.
Avg. Conversation Score: This averages the Overall Conversation Score for all learners who have been assigned the conversation. We only use FULL attempts for this metric since partial attempts could dramatically bring down the average.
Avg. Duration: This averages the simulation duration field for all learners who have been assigned the conversation. We only use FULL attempts for this metric as well.
Attempt Count: This field shows the total attempt count of each learner, including partial and failed attempts. 
Best Overall Metrics: We highlight the best attempt of each learner in the row. If a learner has never completed the simulation to the end (only has made partial attempts), this row will remain blank apart from the attempts count. Once they have at least 1 failed or passed attempt, the rest of the fields will populate. You can hover over any of these metrics to see the average score for the team on this particular metric.
Learner Practice History: If you click on a learner name who has attempted the simulation, you will open a new tab that shows ALL their prior attempts. You can open any attempt to see their debrief page + simulation threads. 
Stage Level Skills Metrics: This portion of the report dynamically shows the % of skills stars each learner earned by simulation Stage. Results are color-coded to indicate areas that learners may be struggling in. Green = 80% or higher of the maximum possible skills points in that stage.  Orange = 50 - 79% of the maximum possible skills points in that stage. Red = 0 - 49%. 
Untagged Skills Metrics: If there are any nodes that you forgot to tag with a skill, but still provided star ratings for, we use a generic 'Untagged' column so that you can still see their performance. We recommend that all Nodes with star ratings be skills-tagged, so if you notice points in this column, it's a good idea to go back and add a tag. 
Note - It's possible for a learner to have passed a PRIOR version of the simulation, and that prior versions may have had different passing criteria. If this scenario occurs, we will honor the success criteria of the simulation at the time that the learner passed the attempt. Learners under this scenario will have an * next to their score to indicate the performance data was from a prior version. In this case, if there are any metrics or skills that were not available/used at that time, those columns will simply show a dash, and their contents won't be included in any averages for that column.

Reports Available to Admins Only
Platform Activity Report
This high-level dashboard provides organization-wide activity metrics for different user types, moments and certifications.

Primary Use: Quick view into the recent usage across the learner side of the platform.

More Information: Individual growth and development is central to our offering, and it rolls up into easy to use team and organization-wide views. We use a flexible, nested learning path design approach that lets you a) define and edit team definitions b) sequence lessons, simulations, paths, and other content. All of this is dynamically captured in our native reporting suite to help you track performance and progress at the organization level. 

 
Platform Activity Report
 
Platform Rollup Stats: The numbers across the top of this report are aggregate numbers for the number of users, coaches, moments, and certification cases in your platform. These likely won't change as often as the data in the rest of your reports. 
Most Active: We've called out the most active Moments and most active teams in the next two panels. 'Most Active' means an attempt of some kind on a moment - for example if a learner makes two attempts on a specific moment in a week, both attempts will be reflected in the 'Most Active' count for that moment that week. 
Date Range: You can filter the report by date range to see activity over time. The default setting is 'Last 7 Days.' Simply click the date range to make adjustments.
Coach Report
This report shows a breakdown of each coach, their interactions and open inbox items. you can drill down to an individual coach to perform quality audits as well!

Primary Use: to see how individual coaches are performing and to determine if coaches need retraining on appropriate coaching/rating or if they need additional resources to help clear out their inbox more quickly for learners.

More Information: The Coach report provides granular data on individual coach activity including:

Which team(s) Coaches are assigned to
How many items they have open in their inbox, broken down by days since the item was submitted (1-7, 8-30, 30+)
How many items they have coached in the last 30 days
How many certifications they have completed in the last 30 days
Screen Shot 2023-06-05 at 9.23.02 AM

You can also use the report to see individual coaching to conduct a quality audit by simply clicking their name.

Individual Coach Report

Once you have selected a coach, you can see their total number of coaching interactions, a breakdown of the star distribution for those interactions, and a record of each coaching they completed. If you expand the individual coaching interactions, you will be able to see the learner's submission as well as any comments left by the coach. This is a great tool to audit your coaches and ensure they are helping to contribute to the highest quality experience for your learners.

AI Training
This report provides an overview of all Audio Recording and Writing moments which have AI rules - both In Training and Live. This data can be used to inform the decision to turn an AI moment Live, and to monitor updates and revisions made by admins while the moments are In Training. 

Primary Use: This data can be used to inform the decision to turn an AI moment Live once it is well-trained and Coaches agree with its ratings.

More Information: The AI Training report shows a total count of how many moments are in training and how many are live. As you have your initial test group of 10-15 people submit against your new AI rules, the data is tracked for the number or rule edits and the acceptance rate. Our standard recommendation is to have at least 10-15 interactions and an acceptance rate of at least 80% before turning the rule live. When you are ready to turn the rule live, simply click the toggle switch to turn the rule live! For more information on training AI rules, check out the article on the Introduction to Bright's AI Coach.




Reporting Suite Overview for Coaches
Coaches in Bright have access to 5 different reports. Review what's available in each, and when to use them.
Reports Available to Coaches
When a coach opens the Reporting Page within Bright, they have 5 reports available to them that we will review below. Note: There is also a helpful report available in the Coach tab. Click here to learn more about that report.

Coach Menu Screenshot

Team Progress
This exportable, detailed report shows the completion percentages for each team in the platform. As you drill down into the tier 4 team, you will see each moment that is part of ANY of the assignments for this group of learners. For each moment, the users will have a designation such as 'Pass' that will be explained below.

Primary Use: Tracking completion and learner progress data at the team or individual level. Export this report into an excel file to easily manipulate the data into the format you need. If you are new to excel, do not fret! We have another article here, with some of the more common functions within excel to make your data clear.

More Information: We have good news for you: No more creating customer completion reports. Our permissions level admins and coaches can view team performance and completion all the way down to the lesson level. It's a powerful view. And honestly, it's fun!
 

Progress Bars (visible on tiers 1-3): If you click on the 'Team Progress' report tab, you can see an overview of completions and activity by Team. There are two basic completion numbers to consider. "% Completed' reflects moments that are truly complete - not just submitted for coach review. To be considered complete a rated moment must be attempted and then rated with a passing score by the coach. If the coach sends that moment back for practice/iteration, that moment will stay noted as 'In Progress.'   
Team Completions (visible on tier 4 level): If you click on a team you can then see more detail on any associated team members, with moment-by-moment real time information about completion status, and the number of attempts they've made on a Moment. The following statuses can be found in the TPR:
Not Started- The learner has not submitted any attempts against the moment.
Pass- The learner has completed at least one passing submission against the moment.
Resubmit- The learner has submitted a response to the moment, but they did not pass and will need to resubmit a new response.
In Progress- The learner submitted a response to the moment, but the response is in the coach inbox pending grading from a human coach. This can occur on live AI moments if the learner exceeded the maximum submissions.
Blank- The moment is included in the table because at least one user on the team has it in their assignment path, but the user showing as blank does not have it within their assignment path.
Scroll right/left to get a high level view of team progress, and to note learners who may be behind (i.e., the gray 'Not Started' entries). If a learner passes a moment on the 1st attempt, we note that completion with a star.
Individual Profile Report
The individual profile report gives key insights into the skills and performance of an individual learner.

Primary Use: This report is perfect when preparing for one-on-one coaching opportunities to guide the conversation in both strengths and weaknesses.

More Information : Through the extensive use of simulation, coaching, tagging, and ratings, we're able to answer these questions in a way few others can. Because Bright measures skill and competency - not just knowledge - we can pinpoint learner strengths and development areas. As a result, we help our customers redirect limited resources to the people, teams, and topics that need them most. We can answer the question 'Are my people ready?' - and if they are not we can quickly point to why. This report is your first step on that journey.

While this isn't the 'first' report you'll see, we consider this report the most important in our reporting suite, so we'll start here. To find the individual skills profile report click the 'Team Progress Report' tab. Once you have drilled down to the tier 4 team the individual is on- this may be the default view if you only coach one tier 4 team, simply click on their name. 

 
Primary Skills Tags: The skills profile report is driven by whatever Primary and Secondary Tags are used on rated Moment types for each user. Across the top of the report you'll see clickable categories of skills. If you click on a skill category, the moment list underneath will change, showing the Moments tagged under that skill tag, and any associated performance data. The star score for the Primary Skills Category is a weighted average rollup of skills in that section. 
Moment Name: Once you click on a skills category, you'll see all rated moments grouped by simulation. If you're looking for performance on a specific moment, you can click the small arrow next to the Moments title to sort, or visit the Moment Report (more on this below)
Secondary Skills Tags: We note the secondary skills tags next to each moment as a reminder of what specific skills were rated. If a moment has multiple tags, the tags will each show in the row, however only the rating for the skill tag associated with the current Primary Skills Category will show. 
Attempt Scores: Each time a learner iterates on a practice moment, their star rating or system duration will be noted in a small graph. For rated moments, ideally you'll see a path of progressive improvement (i.e., the graph goes up) and for system moments you'll see a path of increased proficiency (i.e., the system duration graph goes down). If you hover over a data point you can see more details about that attempt.
 

Moments Report
This detailed report pulls out completion and performance data on one moment at a time. You can review the following:

What % of learners who have not started, are in progress, passed in more than one attempt or passed in one attempt
On Multiple Choice moments, you can see % breakdowns of answers selected by learners for each question
Number of individual submissions for each learner
Primary Use: Use this report to look for patterns and trends in performance, one moment at a time. 

More Information: Since the delivery of practice and coaching is new for most organizations, we blend content and individual reporting views to show how each piece of a simulation performs. For example, if you're building CRM proficiency, you can drill into system simulations and see time proficiency, missed clicks, etc. If you're testing soft skills in management or upselling, you can analyze how many attempts different teams/users are making.
To use this report, begin by selecting the moment name from the alphabetical dropdown list. Then filter by team to view the summary for the moment.
 

 
Moment Details: After you select a Moment from the dropdown to view the report, you can see a recap of the Moment and even click the hyperlink to re-visit the Moment to remind yourself what the exercise and prompts were. 
Attempts Summary: Most moment reports feature a pie chart that notes progress, including either completion status, or performance based on # attempts. Multiple Choice Moments also feature a breakdown of % selections of question answers to support analysis of question difficulty and content effectiveness. System Moments also feature a summary of average attempts and lifelines used for system simulations. 
Team Filter: You can filter moment reports by team to compare and contrast performance across cohorts. This filter can also help serve as a quick 'completion' report in some cases where you'd like to see Team performance on a single lesson. 
Individual Performance Snapshot: At the bottom of each moment report you can see individual learner performance, including status, number of attempts, and - for system moments - duration progress. If you see a graph, hover on data points to see more data about the attempt! 
Outstanding Report
The outstanding report gives a quick breakdown of each learner and the moments they have outstanding (ie. in progress, resubmit, or not started). For a breakdown of what each designation means, review the designation information here.

Primary Use: Use this report to help a group of learners cross the finish line by providing clarity on which moments are not yet complete and providing an estimate of how long it will take to complete these remaining items.

More Information: In the outstanding report, you will see each learner on the team and breakdowns of times and number of outstanding items to complete in their assignment path.

Oustanding Report Example A

Click on a learner's name to view the itemized breakdown of each moment outstanding for the learner. Keep in mind- in progress indicates a submission is awaiting coach feedback in the coach inbox.

Outstanding Report Example B

Assignment Report
Use the assignment report to track to completion of an assignment path for a specific team. This report is generalized and provides a high-level overview of the completion for a specific training assignment.

Primary Use: Quickly report the progress of learners against one assignment path to stakeholders. This report shows each simulation and each moment in the assignment path along with the designation counts for learner progress.

More Information: The assignment report shows an entire assignment path broken into the simulations that make up the assignment.

Assignment Report Example

Each simulation is broken into each comprising moment with its skill tags and the totals for how many learners on the team are assigned each moment. Of the total population, the report will also display the total number of learners who have each designation (ie. in progress, resubmit, or not started). For a breakdown of what each designation means, review the designation information here.

Simulation Skills Report
At the end of each learner's conversation simulation attempt they receive a real-time summary of their performance metrics + targeted coaching around any skills that need more work. This report combines the same metrics into a consolidated view for coaches.

Primary Use: Identify the strengths and opportunities for growth and reinforcement from the full conversation experiences.

More Information: To best understand this report, let's quick remember how the learner experience will conclude on each conversation builder attempt. As soon as the conversation (and possible the system simulation depending on the configuration) is complete, the learner sees this:

Post Simulation Learner Summary

Coaches and Admins can get a rollup view of this for their teams by using the Bright Simulation Skills Report. To access this, go to the 'Reporting' menu tab and select 'Simulation Skills Report' from the report list. 

After you select the team + simulation, you'll see a report that looks like this:

Simulation Skills Report

Key report features include: 

Completion Rate: this calculates the % of learners who have been assigned this conversation that have PASSED the experience. Remember that this report shows all team members, including those who have not been assigned the conversation. We only include those who have been assigned the conversation in the calculation.
Total Practice Time: This aggregates the total number of hours the team has spent practicing this conversation. This number includes BOTH partial and full attempts of the conversation.
Avg. Conversation Score: This averages the Overall Conversation Score for all learners who have been assigned the conversation. We only use FULL attempts for this metric since partial attempts could dramatically bring down the average.
Avg. Duration: This averages the simulation duration field for all learners who have been assigned the conversation. We only use FULL attempts for this metric as well.
Attempt Count: This field shows the total attempt count of each learner, including partial and failed attempts. 
Best Overall Metrics: We highlight the best attempt of each learner in the row. If a learner has never completed the simulation to the end (only has made partial attempts), this row will remain blank apart from the attempts count. Once they have at least 1 failed or passed attempt, the rest of the fields will populate. You can hover over any of these metrics to see the average score for the team on this particular metric.
Learner Practice History: If you click on a learner name who has attempted the simulation, you will open a new tab that shows ALL their prior attempts. You can open any attempt to see their debrief page + simulation threads. 
Stage Level Skills Metrics: This portion of the report dynamically shows the % of skills stars each learner earned by simulation Stage. Results are color-coded to indicate areas that learners may be struggling in. Green = 80% or higher of the maximum possible skills points in that stage.  Orange = 50 - 79% of the maximum possible skills points in that stage. Red = 0 - 49%. 
Untagged Skills Metrics: If there are any nodes that you forgot to tag with a skill, but still provided star ratings for, we use a generic 'Untagged' column so that you can still see their performance. We recommend that all Nodes with star ratings be skills-tagged, so if you notice points in this column, it's a good idea to go back and add a tag.








